{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfffbfae-115a-4b87-b269-8643c598a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import praw\n",
    "import html\n",
    "import ast\n",
    "import re\n",
    "import ftfy\n",
    "from markdown_it import MarkdownIt\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "deaf3714-ac65-4f61-a48a-64fa416fc8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zank7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zank7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zank7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e12a3bc4-91c4-458f-aeb2-a5775fc6d0a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load Spacy medical model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_sci_lg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Spacy medical model\n",
    "nlp = spacy.load(\"en_core_sci_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda34cfe-98c1-4869-8dc2-7f36113ca07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = \"AskDoc Scraper\"\n",
    "posts_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44c03b42-f495-48f7-82bb-2c4a2b16f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = praw.Reddit(\n",
    "    client_id = \"_JTrgh_onvOkRlXXuYk7xQ\",\n",
    "    client_secret = \"yjEhrCabnwnNb8gxXaQGL17Cn2Nsag\",\n",
    "    user_agent = user_agent\n",
    ")\n",
    "subreddit = rd.subreddit('AskDocs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d77a48-bb86-4fa8-88ba-27576105a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(sort_type = \"hot\", limit = 1000):\n",
    "    try:\n",
    "        if sort_type not in ['hot', 'top']:\n",
    "            raise ValueError(\"sort_type must be 'hot' or 'top'\")\n",
    "        posts_sort = subreddit.top(limit = limit) if sort_type == 'top' else subreddit.hot(limit = limit)\n",
    "        for index, post in enumerate(posts_sort):\n",
    "            if index % 10 == 0:\n",
    "                print(f\"Accessed {index} posts\") \n",
    "                \n",
    "            post_data = {\n",
    "                'id': post.id,\n",
    "                'title': post.title,\n",
    "                'selftext': post.selftext,\n",
    "                'author': post.author.name if post.author else None,\n",
    "                'score': post.score,\n",
    "                'num_comments': post.num_comments,\n",
    "                'created_utc': post.created_utc,\n",
    "                'flair': post.link_flair_text\n",
    "            }\n",
    "            \n",
    "            comments_data = []\n",
    "            post.comments.replace_more(limit=0)\n",
    "            for comment in post.comments.list():\n",
    "                comment_data = {\n",
    "                    'id': comment.id,\n",
    "                    'body': comment.body,\n",
    "                    'author': comment.author.name if comment.author else None,\n",
    "                    'score': comment.score,\n",
    "                    'parent_id': comment.parent_id,\n",
    "                    'created_utc': comment.created_utc,\n",
    "                    'is_submitter': comment.is_submitter\n",
    "                }\n",
    "                comments_data.append(comment_data)\n",
    "            \n",
    "            post_data['comments'] = comments_data\n",
    "            posts_data.append(post_data) \n",
    "            \n",
    "            if index % 100 == 0 and index != 0:  \n",
    "                initial_df = pd.DataFrame(posts_data)\n",
    "                initial_df.to_csv(f'reddit_data_{index}.csv', index=False)\n",
    "                print(f\"Data saved at {index} posts\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        initial_df = pd.DataFrame(posts_data)\n",
    "        initial_df.to_csv('reddit_data_error_save.csv', index=False)\n",
    "        raise\n",
    "    \n",
    "    posts_df = pd.DataFrame(posts_data)\n",
    "    posts_df.to_csv(f'AskDoc_{sort_type}data.csv', index = False)\n",
    "    print(\"Succesfully scraped the data\")\n",
    "    return posts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efcd8209-1403-49ce-b0f1-adc182514018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessed 0 posts\n",
      "Accessed 10 posts\n",
      "Accessed 20 posts\n",
      "Accessed 30 posts\n",
      "Accessed 40 posts\n",
      "Accessed 50 posts\n",
      "Accessed 60 posts\n",
      "Accessed 70 posts\n",
      "Accessed 80 posts\n",
      "Accessed 90 posts\n",
      "Accessed 100 posts\n",
      "Data saved at 100 posts\n",
      "Accessed 110 posts\n",
      "Accessed 120 posts\n",
      "Accessed 130 posts\n",
      "Accessed 140 posts\n",
      "Accessed 150 posts\n",
      "Accessed 160 posts\n",
      "Accessed 170 posts\n",
      "Accessed 180 posts\n",
      "Accessed 190 posts\n",
      "Accessed 200 posts\n",
      "Data saved at 200 posts\n",
      "Accessed 210 posts\n",
      "Accessed 220 posts\n",
      "Accessed 230 posts\n",
      "Accessed 240 posts\n",
      "Accessed 250 posts\n",
      "Accessed 260 posts\n",
      "Accessed 270 posts\n",
      "Accessed 280 posts\n",
      "Accessed 290 posts\n",
      "Accessed 300 posts\n",
      "Data saved at 300 posts\n",
      "Accessed 310 posts\n",
      "Accessed 320 posts\n",
      "Accessed 330 posts\n",
      "Accessed 340 posts\n",
      "Accessed 350 posts\n",
      "Accessed 360 posts\n",
      "Accessed 370 posts\n",
      "Accessed 380 posts\n",
      "Accessed 390 posts\n",
      "Accessed 400 posts\n",
      "Data saved at 400 posts\n",
      "Accessed 410 posts\n",
      "Accessed 420 posts\n",
      "Accessed 430 posts\n",
      "Accessed 440 posts\n",
      "Accessed 450 posts\n",
      "Accessed 460 posts\n",
      "Accessed 470 posts\n",
      "Accessed 480 posts\n",
      "Accessed 490 posts\n",
      "Accessed 500 posts\n",
      "Data saved at 500 posts\n",
      "Accessed 510 posts\n",
      "Accessed 520 posts\n",
      "Accessed 530 posts\n",
      "Accessed 540 posts\n",
      "Accessed 550 posts\n",
      "Accessed 560 posts\n",
      "Accessed 570 posts\n",
      "Accessed 580 posts\n",
      "Accessed 590 posts\n",
      "Accessed 600 posts\n",
      "Data saved at 600 posts\n",
      "Accessed 610 posts\n",
      "Accessed 620 posts\n",
      "Accessed 630 posts\n",
      "Accessed 640 posts\n",
      "Accessed 650 posts\n",
      "Accessed 660 posts\n",
      "Accessed 670 posts\n",
      "Accessed 680 posts\n",
      "Accessed 690 posts\n",
      "Accessed 700 posts\n",
      "Data saved at 700 posts\n",
      "Accessed 710 posts\n",
      "Accessed 720 posts\n",
      "Accessed 730 posts\n",
      "Accessed 740 posts\n",
      "Accessed 750 posts\n",
      "Accessed 760 posts\n",
      "Accessed 770 posts\n",
      "Accessed 780 posts\n",
      "Accessed 790 posts\n",
      "Accessed 800 posts\n",
      "Data saved at 800 posts\n",
      "Accessed 810 posts\n",
      "Accessed 820 posts\n",
      "Accessed 830 posts\n",
      "Accessed 840 posts\n",
      "Accessed 850 posts\n",
      "Accessed 860 posts\n",
      "Accessed 870 posts\n",
      "Accessed 880 posts\n",
      "Accessed 890 posts\n",
      "Accessed 900 posts\n",
      "Data saved at 900 posts\n",
      "Accessed 910 posts\n",
      "Accessed 920 posts\n",
      "Accessed 930 posts\n",
      "Accessed 940 posts\n",
      "Accessed 950 posts\n",
      "Accessed 960 posts\n",
      "Accessed 970 posts\n",
      "Accessed 980 posts\n",
      "Accessed 990 posts\n",
      "Succesfully scraped the data\n"
     ]
    }
   ],
   "source": [
    "top_posts = scrape_data(\"top\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc79f418-8e91-4d06-bca0-dbd028b4b2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessed 0 posts\n",
      "Accessed 10 posts\n",
      "Accessed 20 posts\n",
      "Accessed 30 posts\n",
      "Accessed 40 posts\n",
      "Accessed 50 posts\n",
      "Accessed 60 posts\n",
      "Accessed 70 posts\n",
      "Accessed 80 posts\n",
      "Accessed 90 posts\n",
      "Accessed 100 posts\n",
      "Data saved at 100 posts\n",
      "Accessed 110 posts\n",
      "Accessed 120 posts\n",
      "Accessed 130 posts\n",
      "Accessed 140 posts\n",
      "Accessed 150 posts\n",
      "Accessed 160 posts\n",
      "Accessed 170 posts\n",
      "Accessed 180 posts\n",
      "Accessed 190 posts\n",
      "Accessed 200 posts\n",
      "Data saved at 200 posts\n",
      "Accessed 210 posts\n",
      "Accessed 220 posts\n",
      "Accessed 230 posts\n",
      "Accessed 240 posts\n",
      "Accessed 250 posts\n",
      "Accessed 260 posts\n",
      "Accessed 270 posts\n",
      "Accessed 280 posts\n",
      "Accessed 290 posts\n",
      "Accessed 300 posts\n",
      "Data saved at 300 posts\n",
      "Accessed 310 posts\n",
      "Accessed 320 posts\n",
      "Accessed 330 posts\n",
      "Accessed 340 posts\n",
      "Accessed 350 posts\n",
      "Accessed 360 posts\n",
      "Accessed 370 posts\n",
      "Accessed 380 posts\n",
      "Accessed 390 posts\n",
      "Accessed 400 posts\n",
      "Data saved at 400 posts\n",
      "Accessed 410 posts\n",
      "Accessed 420 posts\n",
      "Accessed 430 posts\n",
      "Accessed 440 posts\n",
      "Accessed 450 posts\n",
      "Accessed 460 posts\n",
      "Accessed 470 posts\n",
      "Accessed 480 posts\n",
      "Accessed 490 posts\n",
      "Accessed 500 posts\n",
      "Data saved at 500 posts\n",
      "Accessed 510 posts\n",
      "Accessed 520 posts\n",
      "Accessed 530 posts\n",
      "Accessed 540 posts\n",
      "Accessed 550 posts\n",
      "Accessed 560 posts\n",
      "Accessed 570 posts\n",
      "Accessed 580 posts\n",
      "Accessed 590 posts\n",
      "Accessed 600 posts\n",
      "Data saved at 600 posts\n",
      "Accessed 610 posts\n",
      "Accessed 620 posts\n",
      "Accessed 630 posts\n",
      "Accessed 640 posts\n",
      "Accessed 650 posts\n",
      "Accessed 660 posts\n",
      "Accessed 670 posts\n",
      "Accessed 680 posts\n",
      "Accessed 690 posts\n",
      "Accessed 700 posts\n",
      "Data saved at 700 posts\n",
      "Accessed 710 posts\n",
      "Accessed 720 posts\n",
      "Accessed 730 posts\n",
      "Succesfully scraped the data\n"
     ]
    }
   ],
   "source": [
    "hot_posts = scrape_data(\"hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7c7fa5e-5c95-4010-a230-ee12634a874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('AskDoc_hot_data1.csv')\n",
    "# data1_copy = data1.copy()\n",
    "data2 = pd.read_csv('AskDoc_hot_data2.csv')\n",
    "# data2_copy = data2.copy()\n",
    "data3 = pd.read_csv('AskDoc_topdata.csv')\n",
    "# data3_copy = data3.copy()\n",
    "\n",
    "merged_data = pd.concat([data1, data2, data3], ignore_index = True)\n",
    "merged_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# print(merged_data)\n",
    "# print(len(merged_data))\n",
    "\n",
    "final_data = merged_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf960ff-e716-4d50-a725-60c8dbd40edb",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Notes for later:\n",
    "\n",
    "handle null data\n",
    "\n",
    "handle duplicates\n",
    "\n",
    "Removing mod comments\n",
    "\n",
    "Since hot posts may have unmoderated comment, can't just rely on reddit rules to assume the data's good. Thus, gotta use all sorts of methods to clean the posts and comments before fine tuning the llm. My plan:\n",
    "\n",
    "replace special characters\n",
    "\n",
    "replace html stuff\n",
    "\n",
    "replace encoded characters\n",
    "\n",
    "detect non english language and exclude them\n",
    "\n",
    "spell check\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d78e4535-af9b-462f-b96e-9126f5bc6f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of rows for the scraped data: 6956\n"
     ]
    }
   ],
   "source": [
    "#null data handling\n",
    "print(f\"Initial number of rows for the scraped data: {len(final_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25047614-20b1-4872-a86d-b1e516a59d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdi = MarkdownIt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d662a397-089f-4b9d-857c-1bab1111e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, author):\n",
    "    # Skip processing for bot authors or deleted/removed content\n",
    "    if author in ('AskDocs-ModTeam', 'AutoModerator') or text in ('[removed]', '[deleted]'):\n",
    "        return None \n",
    "\n",
    "    emojis = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    \n",
    "    # Continue processing if not by a bot or deleted/removed\n",
    "    text = mdi.render(text)\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = html.unescape(text)\n",
    "    text = text.lower()  # Normalize case\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = emojis.sub(r'', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "final_data['title'] = final_data.apply(lambda row: clean_text(row['title'], row['author']), axis=1)\n",
    "final_data['selftext'] = final_data.apply(lambda row: clean_text(row['selftext'], row['author']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42c59fb5-cc86-4a57-b69e-c389a658c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comments(comments, author):\n",
    "    comments_list = ast.literal_eval(comments)\n",
    "    cleaned_comments = []\n",
    "    for comment in comments_list:\n",
    "        if comment['author'] not in ('AskDocs-ModTeam', 'AutoModerator') and comment['body'] not in ('[removed]', '[deleted]'):\n",
    "            cleaned_text = clean_text(comment['body'], comment['author'])\n",
    "            if cleaned_text is not None:\n",
    "                cleaned_comments.append(cleaned_text)\n",
    "    return cleaned_comments\n",
    "\n",
    "final_data['comments'] = final_data.apply(lambda row: clean_comments(row['comments'], row['author']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52b985d7-c01f-4228-a80d-80a3d799bc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after dropping null values: 4777\n"
     ]
    }
   ],
   "source": [
    "new_data = final_data.dropna()\n",
    "# final_data = final_data[final_data['comments'].str.len() > 5]\n",
    "print(f\"Number of rows after dropping null values: {len(new_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b82acaf0-0b03-437f-af31-b497001772f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv('CleanData.csv', encoding = 'utf-8', index = False)\n",
    "# new_data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b24c8c89-fdfa-4d46-bae5-ac260c2fbd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>flair</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1cc3km6</td>\n",
       "      <td>mother hospitalized with acute psychosis  is i...</td>\n",
       "      <td>i was informed that my mother and best friend ...</td>\n",
       "      <td>thehattedllama</td>\n",
       "      <td>60</td>\n",
       "      <td>33</td>\n",
       "      <td>1.713978e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[sorry this is happening you dont say your mom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1cbjiqd</td>\n",
       "      <td>what are the actual chances of this happening</td>\n",
       "      <td>my son was stillborn last month when i was 37 ...</td>\n",
       "      <td>InstantElla</td>\n",
       "      <td>425</td>\n",
       "      <td>54</td>\n",
       "      <td>1.713915e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[sorry for your loss my deepest sympathies tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1cbllur</td>\n",
       "      <td>is it weird or unsafe to clean your butthole i...</td>\n",
       "      <td>age 42 sex male height 59 weight 270 race hisp...</td>\n",
       "      <td>tomowudi</td>\n",
       "      <td>228</td>\n",
       "      <td>108</td>\n",
       "      <td>1.713921e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[short answer this is kinda strange but youre ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "1  1cc3km6  mother hospitalized with acute psychosis  is i...   \n",
       "3  1cbjiqd      what are the actual chances of this happening   \n",
       "4  1cbllur  is it weird or unsafe to clean your butthole i...   \n",
       "\n",
       "                                            selftext          author  score  \\\n",
       "1  i was informed that my mother and best friend ...  thehattedllama     60   \n",
       "3  my son was stillborn last month when i was 37 ...     InstantElla    425   \n",
       "4  age 42 sex male height 59 weight 270 race hisp...        tomowudi    228   \n",
       "\n",
       "   num_comments   created_utc                flair  \\\n",
       "1            33  1.713978e+09  Physician Responded   \n",
       "3            54  1.713915e+09  Physician Responded   \n",
       "4           108  1.713921e+09  Physician Responded   \n",
       "\n",
       "                                            comments  \n",
       "1  [sorry this is happening you dont say your mom...  \n",
       "3  [sorry for your loss my deepest sympathies tru...  \n",
       "4  [short answer this is kinda strange but youre ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47200250-1faa-42b7-be3f-f5dc998de570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(text):\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Initialize the WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Load English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Lemmatize tokens and remove stopwords and non-alphabetic tokens\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words\n",
    "    ]\n",
    "    \n",
    "    # Re-join lemmatized tokens into a single string\n",
    "    return ' '.join(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83f06e57-64e8-4cd5-a7f5-e9a78f397623",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = new_data.copy()\n",
    "final_data['title'] = final_data['title'].apply(standardize_text)\n",
    "final_data['selftext'] = final_data['selftext'].apply(standardize_text)\n",
    "final_data['comments'] = final_data['comments'].apply(lambda x: [standardize_text(comment) for comment in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b180c33-7d3c-467f-8c71-1f09624cd4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>flair</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1cc3km6</td>\n",
       "      <td>mother hospitalized acute psychosis possible h...</td>\n",
       "      <td>informed mother best friend taken er last nigh...</td>\n",
       "      <td>thehattedllama</td>\n",
       "      <td>60</td>\n",
       "      <td>33</td>\n",
       "      <td>1.713978e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[sorry happening dont say mom age unusual deve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1cbjiqd</td>\n",
       "      <td>actual chance happening</td>\n",
       "      <td>son stillborn last month week morning felt mov...</td>\n",
       "      <td>InstantElla</td>\n",
       "      <td>425</td>\n",
       "      <td>54</td>\n",
       "      <td>1.713915e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[sorry loss deepest sympathy true umbilical co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1cbllur</td>\n",
       "      <td>weird unsafe clean butthole shower finger</td>\n",
       "      <td>age sex male height weight race hispanic durat...</td>\n",
       "      <td>tomowudi</td>\n",
       "      <td>228</td>\n",
       "      <td>108</td>\n",
       "      <td>1.713921e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[short answer kinda strange youre right harmfu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1cc49rs</td>\n",
       "      <td>seeking adviceavenues explore leg paralysis ag...</td>\n",
       "      <td>month ago woke extreme leg pain point couldnt ...</td>\n",
       "      <td>gemmab14</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1.713980e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[usual disclaimer one provide specific medical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1cca3kx</td>\n",
       "      <td>ultrasound report</td>\n",
       "      <td>male lump right testicle complain increased ur...</td>\n",
       "      <td>mangiafazola</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.713994e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[hello friend im wary firing anything certain ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "1  1cc3km6  mother hospitalized acute psychosis possible h...   \n",
       "3  1cbjiqd                            actual chance happening   \n",
       "4  1cbllur          weird unsafe clean butthole shower finger   \n",
       "5  1cc49rs  seeking adviceavenues explore leg paralysis ag...   \n",
       "7  1cca3kx                                  ultrasound report   \n",
       "\n",
       "                                            selftext          author  score  \\\n",
       "1  informed mother best friend taken er last nigh...  thehattedllama     60   \n",
       "3  son stillborn last month week morning felt mov...     InstantElla    425   \n",
       "4  age sex male height weight race hispanic durat...        tomowudi    228   \n",
       "5  month ago woke extreme leg pain point couldnt ...        gemmab14      9   \n",
       "7  male lump right testicle complain increased ur...    mangiafazola      5   \n",
       "\n",
       "   num_comments   created_utc                flair  \\\n",
       "1            33  1.713978e+09  Physician Responded   \n",
       "3            54  1.713915e+09  Physician Responded   \n",
       "4           108  1.713921e+09  Physician Responded   \n",
       "5             3  1.713980e+09  Physician Responded   \n",
       "7             4  1.713994e+09  Physician Responded   \n",
       "\n",
       "                                            comments  \n",
       "1  [sorry happening dont say mom age unusual deve...  \n",
       "3  [sorry loss deepest sympathy true umbilical co...  \n",
       "4  [short answer kinda strange youre right harmfu...  \n",
       "5  [usual disclaimer one provide specific medical...  \n",
       "7  [hello friend im wary firing anything certain ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261325b-d095-489a-b4ab-d7b0b6921cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_medical_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a5e08-5359-40fa-be4a-c60be234e562",
   "metadata": {},
   "source": [
    "## Data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a3265-c4c0-43b3-bba0-815f00561435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(comment):\n",
    "    return TextBlob(comment).sentiment.polarity\n",
    "\n",
    "# Apply sentiment analysis on a sample comment\n",
    "sample_sentiment = sentiment_analysis(final_data['comments'][500][1])  # Analyze the second comment for example\n",
    "print(\"Sentiment Polarity:\", sample_sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
