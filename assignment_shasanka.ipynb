{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41be0a28-c657-42f1-980f-562b9730f03c",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f366a-8e8b-45b2-b4d7-5b763fc0a294",
   "metadata": {},
   "source": [
    "# Fine Tuning a Language Learning Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0a9f42-add5-44a0-a778-7abd2a5eeeb8",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab1743-b9ec-47bc-a519-5149e7518105",
   "metadata": {},
   "source": [
    "This project aims to fine tune a Language Learning Model (LLM) with data scraped from the AskDocs subreddit so as to make it behave like a doctor responding to medical queries. To achieve this, first, posts and comments from the AskDocs are to be pulled. The pulled data are then cleaned and standardized for the purpose of fine tuning. Then, a few potential LLM candidates are selected and benchmarked using Ollama, and then the appropriate model is selected for fine tuning. Once, fine tuned, the model is then tested and benchmarked again and the metrics are compared with the original model to evaluate its performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e6d5e-f901-4da9-b52e-aa683840b680",
   "metadata": {},
   "source": [
    "### 2. Project Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47fbea-9b52-4c0d-ad38-ee6ffe770785",
   "metadata": {},
   "source": [
    "For the purpose of our project, we will be making use of a number of libraries in python. The libraries to be used have been listed below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfffbfae-115a-4b87-b269-8643c598a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import praw\n",
    "import html\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "import ftfy\n",
    "from markdown_it import MarkdownIt\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda34cfe-98c1-4869-8dc2-7f36113ca07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = \"Doc\"\n",
    "posts_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44c03b42-f495-48f7-82bb-2c4a2b16f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = praw.Reddit(\n",
    "    client_id = \"Wyzdr8dkNuFVp0_EwSMuoA\",\n",
    "    client_secret = \"f0uBJYRmNyb_ENBm4ACOolUlZV44Zg\",\n",
    "    user_agent = user_agent\n",
    ")\n",
    "subreddit = rd.subreddit('AskDocs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37d77a48-bb86-4fa8-88ba-27576105a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(sort_type='hot', limit=1000, top_comments=10):\n",
    "    try:\n",
    "        if sort_type not in ['hot', 'top']:\n",
    "            raise ValueError(\"sort_type must be 'hot' or 'top'\")\n",
    "\n",
    "        posts_sort = subreddit.top(limit=limit) if sort_type == 'top' else subreddit.hot(limit=limit)\n",
    "\n",
    "        for index, post in enumerate(posts_sort):\n",
    "            if index % 10 == 0:\n",
    "                print(f\"Accessed {index} posts\")\n",
    "\n",
    "            post_data = {\n",
    "                'id': post.id,\n",
    "                'title': post.title,\n",
    "                'selftext': post.selftext,\n",
    "                'author': post.author.name if post.author else None,\n",
    "                'score': post.score,\n",
    "                'num_comments': post.num_comments,\n",
    "                'created_utc': post.created_utc,\n",
    "                'flair': post.link_flair_text\n",
    "            }\n",
    "\n",
    "            comments_data = []\n",
    "\n",
    "            post.comments.replace_more(limit=0)\n",
    "            comments = sorted([comment for comment in post.comments.list() if comment.author and not comment.author.name.startswith(('AutoModerator', 'AskDocs-ModTeam'))], key=lambda x: x.score, reverse=True)[:top_comments]\n",
    "\n",
    "            for comment in comments:\n",
    "                comment_data = {\n",
    "                    'id': comment.id,\n",
    "                    'body': comment.body,\n",
    "                    'author': comment.author.name if comment.author else None,\n",
    "                    'score': comment.score,\n",
    "                    'parent_id': comment.parent_id,\n",
    "                    'created_utc': comment.created_utc,\n",
    "                    'is_submitter': comment.is_submitter,\n",
    "                    'author_flair': comment.author_flair_text if comment.author_flair_text else None\n",
    "                }\n",
    "                comments_data.append(comment_data)\n",
    "\n",
    "            post_data['comments'] = comments_data\n",
    "            posts_data.append(post_data)\n",
    "\n",
    "            if index % 100 == 0 and index != 0:\n",
    "                initial_df = pd.DataFrame(posts_data)\n",
    "                initial_df.to_csv(f'reddit_data_{index}.csv', index=False)\n",
    "                print(f\"Data saved at {index} posts\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        initial_df = pd.DataFrame(posts_data)\n",
    "        initial_df.to_csv('reddit_data_error_save.csv', index=False)\n",
    "        raise\n",
    "\n",
    "    posts_df = pd.DataFrame(posts_data)\n",
    "    posts_df.to_csv(f'ScrapeAskDoc_{sort_type}data.csv', index=False)\n",
    "    print(\"Successfully scraped the data\")\n",
    "    return posts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f9f39-7405-46ec-8e81-d62175d7c390",
   "metadata": {},
   "source": [
    "Scraping the top posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "efcd8209-1403-49ce-b0f1-adc182514018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessed 0 posts\n",
      "Accessed 10 posts\n",
      "Accessed 20 posts\n",
      "Accessed 30 posts\n",
      "Accessed 40 posts\n",
      "Accessed 50 posts\n",
      "Accessed 60 posts\n",
      "Accessed 70 posts\n",
      "Accessed 80 posts\n",
      "Accessed 90 posts\n",
      "Accessed 100 posts\n",
      "Data saved at 100 posts\n",
      "Accessed 110 posts\n",
      "Accessed 120 posts\n",
      "Accessed 130 posts\n",
      "Accessed 140 posts\n",
      "Accessed 150 posts\n",
      "Accessed 160 posts\n",
      "Accessed 170 posts\n",
      "Accessed 180 posts\n",
      "Accessed 190 posts\n",
      "Accessed 200 posts\n",
      "Data saved at 200 posts\n",
      "Accessed 210 posts\n",
      "Accessed 220 posts\n",
      "Accessed 230 posts\n",
      "Accessed 240 posts\n",
      "Accessed 250 posts\n",
      "Accessed 260 posts\n",
      "Accessed 270 posts\n",
      "Accessed 280 posts\n",
      "Accessed 290 posts\n",
      "Accessed 300 posts\n",
      "Data saved at 300 posts\n",
      "Accessed 310 posts\n",
      "Accessed 320 posts\n",
      "Accessed 330 posts\n",
      "Accessed 340 posts\n",
      "Accessed 350 posts\n",
      "Accessed 360 posts\n",
      "Accessed 370 posts\n",
      "Accessed 380 posts\n",
      "Accessed 390 posts\n",
      "Accessed 400 posts\n",
      "Data saved at 400 posts\n",
      "Accessed 410 posts\n",
      "Accessed 420 posts\n",
      "Accessed 430 posts\n",
      "Accessed 440 posts\n",
      "Accessed 450 posts\n",
      "Accessed 460 posts\n",
      "Accessed 470 posts\n",
      "Accessed 480 posts\n",
      "Accessed 490 posts\n",
      "Accessed 500 posts\n",
      "Data saved at 500 posts\n",
      "Accessed 510 posts\n",
      "Accessed 520 posts\n",
      "Accessed 530 posts\n",
      "Accessed 540 posts\n",
      "Accessed 550 posts\n",
      "Accessed 560 posts\n",
      "Accessed 570 posts\n",
      "Accessed 580 posts\n",
      "Accessed 590 posts\n",
      "Accessed 600 posts\n",
      "Data saved at 600 posts\n",
      "Accessed 610 posts\n",
      "Accessed 620 posts\n",
      "Accessed 630 posts\n",
      "Accessed 640 posts\n",
      "Accessed 650 posts\n",
      "Accessed 660 posts\n",
      "Accessed 670 posts\n",
      "Accessed 680 posts\n",
      "Accessed 690 posts\n",
      "Accessed 700 posts\n",
      "Data saved at 700 posts\n",
      "Accessed 710 posts\n",
      "Accessed 720 posts\n",
      "Accessed 730 posts\n",
      "Accessed 740 posts\n",
      "Accessed 750 posts\n",
      "Accessed 760 posts\n",
      "Accessed 770 posts\n",
      "Accessed 780 posts\n",
      "Accessed 790 posts\n",
      "Accessed 800 posts\n",
      "Data saved at 800 posts\n",
      "Accessed 810 posts\n",
      "Accessed 820 posts\n",
      "Accessed 830 posts\n",
      "Accessed 840 posts\n",
      "Accessed 850 posts\n",
      "Accessed 860 posts\n",
      "Accessed 870 posts\n",
      "Accessed 880 posts\n",
      "Accessed 890 posts\n",
      "Accessed 900 posts\n",
      "Data saved at 900 posts\n",
      "Accessed 910 posts\n",
      "Accessed 920 posts\n",
      "Accessed 930 posts\n",
      "Accessed 940 posts\n",
      "Accessed 950 posts\n",
      "Accessed 960 posts\n",
      "Accessed 970 posts\n",
      "Accessed 980 posts\n",
      "Accessed 990 posts\n",
      "Successfully scraped the data\n"
     ]
    }
   ],
   "source": [
    "top_posts = scrape_data(\"top\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb3cc4-101f-476e-9726-d01b6a06d5d6",
   "metadata": {},
   "source": [
    "Scraping the hot posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc79f418-8e91-4d06-bca0-dbd028b4b2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessed 0 posts\n",
      "Accessed 10 posts\n",
      "Accessed 20 posts\n",
      "Accessed 30 posts\n",
      "Accessed 40 posts\n",
      "Accessed 50 posts\n",
      "Accessed 60 posts\n",
      "Accessed 70 posts\n",
      "Accessed 80 posts\n",
      "Accessed 90 posts\n",
      "Accessed 100 posts\n",
      "Data saved at 100 posts\n",
      "Accessed 110 posts\n",
      "Accessed 120 posts\n",
      "Accessed 130 posts\n",
      "Accessed 140 posts\n",
      "Accessed 150 posts\n",
      "Accessed 160 posts\n",
      "Accessed 170 posts\n",
      "Accessed 180 posts\n",
      "Accessed 190 posts\n",
      "Accessed 200 posts\n",
      "Data saved at 200 posts\n",
      "Accessed 210 posts\n",
      "Accessed 220 posts\n",
      "Accessed 230 posts\n",
      "Accessed 240 posts\n",
      "Accessed 250 posts\n",
      "Accessed 260 posts\n",
      "Accessed 270 posts\n",
      "Accessed 280 posts\n",
      "Accessed 290 posts\n",
      "Accessed 300 posts\n",
      "Data saved at 300 posts\n",
      "Accessed 310 posts\n",
      "Accessed 320 posts\n",
      "Accessed 330 posts\n",
      "Accessed 340 posts\n",
      "Accessed 350 posts\n",
      "Accessed 360 posts\n",
      "Accessed 370 posts\n",
      "Accessed 380 posts\n",
      "Accessed 390 posts\n",
      "Accessed 400 posts\n",
      "Data saved at 400 posts\n",
      "Accessed 410 posts\n",
      "Accessed 420 posts\n",
      "Accessed 430 posts\n",
      "Accessed 440 posts\n",
      "Accessed 450 posts\n",
      "Accessed 460 posts\n",
      "Accessed 470 posts\n",
      "Accessed 480 posts\n",
      "Accessed 490 posts\n",
      "Accessed 500 posts\n",
      "Data saved at 500 posts\n",
      "Accessed 510 posts\n",
      "Accessed 520 posts\n",
      "Accessed 530 posts\n",
      "Accessed 540 posts\n",
      "Accessed 550 posts\n",
      "Accessed 560 posts\n",
      "Accessed 570 posts\n",
      "Accessed 580 posts\n",
      "Accessed 590 posts\n",
      "Accessed 600 posts\n",
      "Data saved at 600 posts\n",
      "Accessed 610 posts\n",
      "Accessed 620 posts\n",
      "Accessed 630 posts\n",
      "Accessed 640 posts\n",
      "Accessed 650 posts\n",
      "Accessed 660 posts\n",
      "Accessed 670 posts\n",
      "Accessed 680 posts\n",
      "Accessed 690 posts\n",
      "Accessed 700 posts\n",
      "Data saved at 700 posts\n",
      "Accessed 710 posts\n",
      "Accessed 720 posts\n",
      "Successfully scraped the data\n"
     ]
    }
   ],
   "source": [
    "hot_posts = scrape_data(\"hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c7fa5e-5c95-4010-a230-ee12634a874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('ScrapeAskDoc_topdata.csv')\n",
    "# data1_copy = data1.copy()\n",
    "data2 = pd.read_csv('ScrapeAskDoc_topdata.csv')\n",
    "# data2_copy = data2.copy()\n",
    "data3 = pd.read_csv('AskDoc_topdata.csv')\n",
    "# data3_copy = data3.copy()\n",
    "\n",
    "#merged_data = pd.concat([data1, data2, data3], ignore_index = True)\n",
    "merged_data = pd.concat([data1, data2])\n",
    "merged_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "#final_data = merged_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07d2cf6-6f0d-4344-bc69-0b9095e1fb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>flair</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1cfva9b</td>\n",
       "      <td>Weekly Discussion/General Questions Thread - A...</td>\n",
       "      <td>**This is a weekly general discussion and gene...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>1.714385e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': 'l1tov1a', 'body': 'say you’re there t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ck71u1</td>\n",
       "      <td>My nonverbal son was given 1000 mg ketamine pr...</td>\n",
       "      <td>My (21 m) son who is 6ft and weighs approximat...</td>\n",
       "      <td>Odd-Magician-3397</td>\n",
       "      <td>176</td>\n",
       "      <td>28</td>\n",
       "      <td>1.714848e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[{'id': 'l2l0syr', 'body': \"Was this all at on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ckgzxa</td>\n",
       "      <td>Why do I get sick when I don't drink milk?</td>\n",
       "      <td>Caucasian male 22, height 5'11, 145lb/65.7kg \\...</td>\n",
       "      <td>Creative-Yak-8287</td>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>1.714876e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[{'id': 'l2n7h1x', 'body': 'There is unlikely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1ckg0mk</td>\n",
       "      <td>Elderly 90+ dad CHF, organ failure - is this c...</td>\n",
       "      <td>EDIT: you all are so sweet and honest and than...</td>\n",
       "      <td>seaw33dthrowaway</td>\n",
       "      <td>36</td>\n",
       "      <td>35</td>\n",
       "      <td>1.714873e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[{'id': 'l2mpqx8', 'body': 'Have you spoken at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1cjxy7q</td>\n",
       "      <td>My friend [36M] is sending 30k to a girl [21F]...</td>\n",
       "      <td>Exam: MRI BRAIN/PITUITARY W/WO\\nDate of Exam: ...</td>\n",
       "      <td>ProbablyShouldStop</td>\n",
       "      <td>278</td>\n",
       "      <td>46</td>\n",
       "      <td>1.714821e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[{'id': 'l2j83zd', 'body': 'She sent him a 3.5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  1cfva9b  Weekly Discussion/General Questions Thread - A...   \n",
       "1  1ck71u1  My nonverbal son was given 1000 mg ketamine pr...   \n",
       "2  1ckgzxa         Why do I get sick when I don't drink milk?   \n",
       "3  1ckg0mk  Elderly 90+ dad CHF, organ failure - is this c...   \n",
       "4  1cjxy7q  My friend [36M] is sending 30k to a girl [21F]...   \n",
       "\n",
       "                                            selftext              author  \\\n",
       "0  **This is a weekly general discussion and gene...       AutoModerator   \n",
       "1  My (21 m) son who is 6ft and weighs approximat...   Odd-Magician-3397   \n",
       "2  Caucasian male 22, height 5'11, 145lb/65.7kg \\...   Creative-Yak-8287   \n",
       "3  EDIT: you all are so sweet and honest and than...    seaw33dthrowaway   \n",
       "4  Exam: MRI BRAIN/PITUITARY W/WO\\nDate of Exam: ...  ProbablyShouldStop   \n",
       "\n",
       "   score  num_comments   created_utc                flair  \\\n",
       "0      1            71  1.714385e+09                  NaN   \n",
       "1    176            28  1.714848e+09  Physician Responded   \n",
       "2     42            13  1.714876e+09  Physician Responded   \n",
       "3     36            35  1.714873e+09  Physician Responded   \n",
       "4    278            46  1.714821e+09  Physician Responded   \n",
       "\n",
       "                                            comments  \n",
       "0  [{'id': 'l1tov1a', 'body': 'say you’re there t...  \n",
       "1  [{'id': 'l2l0syr', 'body': \"Was this all at on...  \n",
       "2  [{'id': 'l2n7h1x', 'body': 'There is unlikely ...  \n",
       "3  [{'id': 'l2mpqx8', 'body': 'Have you spoken at...  \n",
       "4  [{'id': 'l2j83zd', 'body': 'She sent him a 3.5...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = pd.read_csv('ScrapeAskDoc_hotdata.csv')\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf960ff-e716-4d50-a725-60c8dbd40edb",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Notes for later:\n",
    "\n",
    "handle null data\n",
    "\n",
    "handle duplicates\n",
    "\n",
    "Removing mod comments\n",
    "\n",
    "Since hot posts may have unmoderated comment, can't just rely on reddit rules to assume the data's good. Thus, gotta use all sorts of methods to clean the posts and comments before fine tuning the llm. My plan:\n",
    "\n",
    "replace special characters\n",
    "\n",
    "replace html stuff\n",
    "\n",
    "replace encoded characters\n",
    "\n",
    "detect non english language and exclude them\n",
    "\n",
    "spell check\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d78e4535-af9b-462f-b96e-9126f5bc6f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of rows for the scraped data: 500\n"
     ]
    }
   ],
   "source": [
    "#null data handling\n",
    "final_data = pd.read_csv('ScrapeAskDoc_hotdata.csv')\n",
    "print(f\"Initial number of rows for the scraped data: {len(final_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25047614-20b1-4872-a86d-b1e516a59d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdi = MarkdownIt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d662a397-089f-4b9d-857c-1bab1111e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old clean text\n",
    "def clean_text(text, author):\n",
    "    # Skip processing for bot authors or deleted/removed content\n",
    "    if author in ('AskDocs-ModTeam', 'AutoModerator') or text in ('[removed]', '[deleted]'):\n",
    "        return None \n",
    "\n",
    "    emojis = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    \n",
    "    # Continue processing if not by a bot or deleted/removed\n",
    "    text = mdi.render(text)\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = html.unescape(text)\n",
    "    text = text.lower()  # Normalize case\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    #text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = emojis.sub(r'', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "final_data['title'] = final_data.apply(lambda row: clean_text(row['title'], row['author']), axis=1)\n",
    "final_data['selftext'] = final_data.apply(lambda row: clean_text(row['selftext'], row['author']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0253715c-4e84-4d4d-8f4e-0adb57a3939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['comments']\n",
    "final_data.to_csv('check.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29869c33-97fc-4101-ae8d-a365c7e8ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "##old top max score top comment code\n",
    "def clean_comments(comments, author):\n",
    "    comments_list = ast.literal_eval(comments)\n",
    "\n",
    "    # Find the top comment based on the maximum score\n",
    "    top_comment = max(comments_list, key=lambda x: x['score']) if comments_list else None\n",
    "\n",
    "    # Filter out comments from AutoModerator, AskDocs-ModTeam, and removed/deleted comments\n",
    "    if top_comment and top_comment['author'] not in ('AskDocs-ModTeam', 'AutoModerator') and top_comment['body'] not in ('[removed]', '[deleted]'):\n",
    "        cleaned_text = clean_text(top_comment['body'], top_comment['author'])\n",
    "        if cleaned_text is not None:\n",
    "            return [cleaned_text]\n",
    "\n",
    "    return ''\n",
    "\n",
    "final_data['comments'] = final_data.apply(lambda row: clean_comments(row['comments'], row['author']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f60e6708-2a03-4c9c-929b-8d5db8c8543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, author):\n",
    "    if author in ('AskDocs-ModTeam', 'AutoModerator') or text in ('[removed]', '[deleted]'):\n",
    "        return None\n",
    "\n",
    "    emojis = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "\n",
    "    # Normalize the case, remove URLs and extra whitespace, and strip emojis\n",
    "    text = html.unescape(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = emojis.sub(r'', text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9860863-1ff6-46c5-ac89-1420aa54bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comments(comments, author):\n",
    "    comments_list = ast.literal_eval(comments)\n",
    "    cleaned_comments = []\n",
    "\n",
    "    # Process each comment\n",
    "    for comment in comments_list:\n",
    "        if comment['author'] not in ('AskDocs-ModTeam', 'AutoModerator') and comment['body'] not in ('[removed]', '[deleted]'):\n",
    "            cleaned_text = clean_text(comment['body'], comment['author'])\n",
    "            if cleaned_text is not None:\n",
    "                cleaned_comments.append(cleaned_text)\n",
    "\n",
    "    return cleaned_comments\n",
    "\n",
    "# Applying the function to the DataFrame\n",
    "final_data['comments'] = final_data.apply(lambda row: clean_comments(row['comments'], row['author']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52b985d7-c01f-4228-a80d-80a3d799bc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after dropping null values: 121\n"
     ]
    }
   ],
   "source": [
    "new_data = final_data.copy()\n",
    "new_data = new_data.dropna()\n",
    "#new_data = new_data.drop_duplicates()\n",
    "# final_data = final_data[final_data['comments'].str.len() > 5]\n",
    "print(f\"Number of rows after dropping null values: {len(new_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b82acaf0-0b03-437f-af31-b497001772f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>flair</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ck71u1</td>\n",
       "      <td>My nonverbal son was given 1000 mg ketamine pr...</td>\n",
       "      <td>My (21 m) son who is 6ft and weighs approximat...</td>\n",
       "      <td>Odd-Magician-3397</td>\n",
       "      <td>176</td>\n",
       "      <td>28</td>\n",
       "      <td>1.714848e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[was this all at once, or as two separate inje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ckgzxa</td>\n",
       "      <td>Why do I get sick when I don't drink milk?</td>\n",
       "      <td>Caucasian male 22, height 5'11, 145lb/65.7kg \\...</td>\n",
       "      <td>Creative-Yak-8287</td>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>1.714876e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[there is unlikely to be anything physically w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1ckg0mk</td>\n",
       "      <td>Elderly 90+ dad CHF, organ failure - is this c...</td>\n",
       "      <td>EDIT: you all are so sweet and honest and than...</td>\n",
       "      <td>seaw33dthrowaway</td>\n",
       "      <td>36</td>\n",
       "      <td>35</td>\n",
       "      <td>1.714873e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[have you spoken at all with the doctors about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1cjxy7q</td>\n",
       "      <td>My friend [36M] is sending 30k to a girl [21F]...</td>\n",
       "      <td>Exam: MRI BRAIN/PITUITARY W/WO\\nDate of Exam: ...</td>\n",
       "      <td>ProbablyShouldStop</td>\n",
       "      <td>278</td>\n",
       "      <td>46</td>\n",
       "      <td>1.714821e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[she sent him a 3.5yr old mri? i have doubts a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1ckb874</td>\n",
       "      <td>12 month old unresponsive, stumped hospital. A...</td>\n",
       "      <td>12 months old\\n\\nFemale\\n\\nNo medications\\n\\nL...</td>\n",
       "      <td>Babyd2hardrn</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>1.714859e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[all of her bloods are relatively normal (even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>1cjztlt</td>\n",
       "      <td>Is it bad that I drink to much water?</td>\n",
       "      <td>I work 12 hour shifts at my job. When I’m at w...</td>\n",
       "      <td>PossiblyAburd</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.714828e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[sounds pretty normal. i also work 12 hour shi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>1cisrya</td>\n",
       "      <td>Slurred speech continued in 4 year old</td>\n",
       "      <td>4M. 52 pounds. \\n\\nI posted the other day abou...</td>\n",
       "      <td>lolly1997</td>\n",
       "      <td>905</td>\n",
       "      <td>312</td>\n",
       "      <td>1.714689e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[did they do a lumbar puncture (a “spinal tap”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1cjy1o5</td>\n",
       "      <td>Penis atrophy/shrinkage question</td>\n",
       "      <td>\\n\\n20 year old male\\n\\nSo since my prostate p...</td>\n",
       "      <td>Jankis2000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.714822e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[fascinating! there is this strange phenomenon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1cjxnku</td>\n",
       "      <td>What are the chances my baby has birth defects</td>\n",
       "      <td>28F 5’4” 180lbs 4ish weeks pregnant\\n\\nI had a...</td>\n",
       "      <td>because-throw</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.714820e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[if you're only about 4 weeks pregnant now, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1cjpg2u</td>\n",
       "      <td>Peeing 17 times a day</td>\n",
       "      <td>18 years old \\nHeight: 162 cm.\\nWeight: ~ 50kg...</td>\n",
       "      <td>JoeyPollandSmith</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.714789e+09</td>\n",
       "      <td>Physician Responded</td>\n",
       "      <td>[have you had a blood glucose test? any urine ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "1    1ck71u1  My nonverbal son was given 1000 mg ketamine pr...   \n",
       "2    1ckgzxa         Why do I get sick when I don't drink milk?   \n",
       "3    1ckg0mk  Elderly 90+ dad CHF, organ failure - is this c...   \n",
       "4    1cjxy7q  My friend [36M] is sending 30k to a girl [21F]...   \n",
       "6    1ckb874  12 month old unresponsive, stumped hospital. A...   \n",
       "..       ...                                                ...   \n",
       "471  1cjztlt              Is it bad that I drink to much water?   \n",
       "493  1cisrya            Slurred speech continued in 4 year old    \n",
       "496  1cjy1o5                   Penis atrophy/shrinkage question   \n",
       "498  1cjxnku     What are the chances my baby has birth defects   \n",
       "499  1cjpg2u                              Peeing 17 times a day   \n",
       "\n",
       "                                              selftext              author  \\\n",
       "1    My (21 m) son who is 6ft and weighs approximat...   Odd-Magician-3397   \n",
       "2    Caucasian male 22, height 5'11, 145lb/65.7kg \\...   Creative-Yak-8287   \n",
       "3    EDIT: you all are so sweet and honest and than...    seaw33dthrowaway   \n",
       "4    Exam: MRI BRAIN/PITUITARY W/WO\\nDate of Exam: ...  ProbablyShouldStop   \n",
       "6    12 months old\\n\\nFemale\\n\\nNo medications\\n\\nL...        Babyd2hardrn   \n",
       "..                                                 ...                 ...   \n",
       "471  I work 12 hour shifts at my job. When I’m at w...       PossiblyAburd   \n",
       "493  4M. 52 pounds. \\n\\nI posted the other day abou...           lolly1997   \n",
       "496  \\n\\n20 year old male\\n\\nSo since my prostate p...          Jankis2000   \n",
       "498  28F 5’4” 180lbs 4ish weeks pregnant\\n\\nI had a...       because-throw   \n",
       "499  18 years old \\nHeight: 162 cm.\\nWeight: ~ 50kg...    JoeyPollandSmith   \n",
       "\n",
       "     score  num_comments   created_utc                flair  \\\n",
       "1      176            28  1.714848e+09  Physician Responded   \n",
       "2       42            13  1.714876e+09  Physician Responded   \n",
       "3       36            35  1.714873e+09  Physician Responded   \n",
       "4      278            46  1.714821e+09  Physician Responded   \n",
       "6       26            10  1.714859e+09  Physician Responded   \n",
       "..     ...           ...           ...                  ...   \n",
       "471      1             2  1.714828e+09  Physician Responded   \n",
       "493    905           312  1.714689e+09  Physician Responded   \n",
       "496      1             5  1.714822e+09  Physician Responded   \n",
       "498      1             2  1.714820e+09  Physician Responded   \n",
       "499      4             4  1.714789e+09  Physician Responded   \n",
       "\n",
       "                                              comments  \n",
       "1    [was this all at once, or as two separate inje...  \n",
       "2    [there is unlikely to be anything physically w...  \n",
       "3    [have you spoken at all with the doctors about...  \n",
       "4    [she sent him a 3.5yr old mri? i have doubts a...  \n",
       "6    [all of her bloods are relatively normal (even...  \n",
       "..                                                 ...  \n",
       "471  [sounds pretty normal. i also work 12 hour shi...  \n",
       "493  [did they do a lumbar puncture (a “spinal tap”...  \n",
       "496  [fascinating! there is this strange phenomenon...  \n",
       "498  [if you're only about 4 weeks pregnant now, th...  \n",
       "499  [have you had a blood glucose test? any urine ...  \n",
       "\n",
       "[121 rows x 9 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.to_csv('LatestCleaned.csv', encoding = 'utf-8', index = False)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60bf722e-e987-40d1-b5b8-8f94c56b409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data = new_data.drop_duplicates(subset = 'id')\n",
    "# # new_data.head(26)\n",
    "# for x in new_data['comments']:\n",
    "#     print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "403a6b7b-8a2d-4414-9116-ffaa4dea83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv('CleanLatest.csv', encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "213436d2-6bba-4c46-9dab-f8858f116d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b24c8c89-fdfa-4d46-bae5-ac260c2fbd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry this is happening you dont say your moms age but its unusual to develop schizophrenia later in life there are many other things that can cause auditory and visual hallucinations that arent schizophrenia some of which are more likely than schizophrenia depending on her age although having a sibling with it does increase her risk alcohol use disorder can predispose someone to certain types of brain dysfunction and dementia that could look like this too its also worth noting that a ct scan cannot rule in or out schizophrenia its hard to speculate further based on the information you have\n"
     ]
    }
   ],
   "source": [
    "# new_data = pd.read_csv('CleanData.csv')\n",
    "# new_data.head(3)\n",
    "for x in new_data['comments']:\n",
    "    print(x[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47200250-1faa-42b7-be3f-f5dc998de570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Initialize the WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Lemmatize tokens and non-alphabetic tokens\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(token) for token in tokens if token.isalpha()\n",
    "    ]\n",
    "    \n",
    "    # Re-join lemmatized tokens into a single string\n",
    "    return ' '.join(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83f06e57-64e8-4cd5-a7f5-e9a78f397623",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_data = new_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b37f2fee-761a-48be-93bc-3c01b8de2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_data['title'] = stand_data['title'].apply(standardize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2229baf3-1c8a-4f63-9d5f-1cc32de51901",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_data['selftext'] = stand_data['selftext'].apply(standardize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8737ef08-b3c4-46f2-91f2-0b92b84d5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_data['comments'] = stand_data['comments'].apply(lambda x: [standardize_text(comment) for comment in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82950984-63f0-438a-b64d-16e9b3582081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry this is happening you dont say your moms age but its unusual to develop schizophrenia later in life there are many other things that can cause auditory and visual hallucinations that arent schizophrenia some of which are more likely than schizophrenia depending on her age although having a sibling with it does increase her risk alcohol use disorder can predispose someone to certain types of brain dysfunction and dementia that could look like this too its also worth noting that a ct scan cannot rule in or out schizophrenia its hard to speculate further based on the information you have\n"
     ]
    }
   ],
   "source": [
    "for x in new_data['comments']:\n",
    "    print(x[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b180c33-7d3c-467f-8c71-1f09624cd4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>mother hospitalized with acute psychosis is it...</td>\n",
       "      <td>i wa informed that my mother and best friend w...</td>\n",
       "      <td>60</td>\n",
       "      <td>1.713978e+09</td>\n",
       "      <td>['sorry this is happening you dont say your mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>what are the actual chance of this happening</td>\n",
       "      <td>my son wa stillborn last month when i wa week ...</td>\n",
       "      <td>425</td>\n",
       "      <td>1.713915e+09</td>\n",
       "      <td>['sorry for your loss my deepest sympathy true...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>is it weird or unsafe to clean your butthole i...</td>\n",
       "      <td>age sex male height weight race hispanic durat...</td>\n",
       "      <td>228</td>\n",
       "      <td>1.713921e+09</td>\n",
       "      <td>['short answer this is kinda strange but youre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>seeking adviceavenues to explore leg paralysis...</td>\n",
       "      <td>month ago i woke up with extreme leg pain to t...</td>\n",
       "      <td>9</td>\n",
       "      <td>1.713980e+09</td>\n",
       "      <td>['usual disclaimer no one can provide specific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>ultrasound report</td>\n",
       "      <td>male lump on right testicle complain about inc...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.713994e+09</td>\n",
       "      <td>['hello my friend im wary of firing off anythi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           1  mother hospitalized with acute psychosis is it...   \n",
       "1           3       what are the actual chance of this happening   \n",
       "2           4  is it weird or unsafe to clean your butthole i...   \n",
       "3           5  seeking adviceavenues to explore leg paralysis...   \n",
       "4           7                                  ultrasound report   \n",
       "\n",
       "                                            selftext  score   created_utc  \\\n",
       "0  i wa informed that my mother and best friend w...     60  1.713978e+09   \n",
       "1  my son wa stillborn last month when i wa week ...    425  1.713915e+09   \n",
       "2  age sex male height weight race hispanic durat...    228  1.713921e+09   \n",
       "3  month ago i woke up with extreme leg pain to t...      9  1.713980e+09   \n",
       "4  male lump on right testicle complain about inc...      5  1.713994e+09   \n",
       "\n",
       "                                            comments  \n",
       "0  ['sorry this is happening you dont say your mo...  \n",
       "1  ['sorry for your loss my deepest sympathy true...  \n",
       "2  ['short answer this is kinda strange but youre...  \n",
       "3  ['usual disclaimer no one can provide specific...  \n",
       "4  ['hello my friend im wary of firing off anythi...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_data.drop(columns = ['author', 'num_comments', 'flair', 'id'])\n",
    "# stand_data.drop(columns = ['author', 'num_comments', 'flair', 'id'], inplace = True)\n",
    "# stand_data.to_csv('LatestStandardizedData.csv')\n",
    "stand_data = pd.read_csv('LatestStandardizedData.csv')\n",
    "# stand_data['comment'] = stand_data['comments'].apply(lambda x: x[0] if x else '')\n",
    "# stand_data.head(2)\n",
    "\n",
    "stand_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7781e0f-3ac5-45ff-8b9b-307f88ececb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instruction(row):\n",
    "    return \"Based on the medical information provided, generate an appropriate medical response.\"\n",
    "\n",
    "# Combining title and selftext to form the input\n",
    "stand_data['input'] = stand_data['title'] + \" \" + stand_data['selftext']\n",
    "\n",
    "#first comment is a potential output\n",
    "stand_data['output'] = stand_data['comments'].apply(lambda x: eval(x)[0] if x and eval(x) else '')\n",
    "\n",
    "# Adding instruction to each entry\n",
    "stand_data['instruction'] = stand_data.apply(create_instruction, axis=1)\n",
    "\n",
    "# Select relevant columns for the new dataset\n",
    "instruction_set = stand_data[['instruction', 'input', 'output']]\n",
    "\n",
    "# Save the transformed dataset\n",
    "instruction_set.to_csv('instruction_set.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "179f7113-d691-4c68-8118-0c0d96874235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 22:38:30,025 - INFO - Initializing MedicalSpecialtyPipeline with model: MoritzLaurer/deberta-v3-large-zeroshot-v2.0\n",
      "2024-05-06 22:38:37,156 - INFO - Processing dataframe with 5 rows\n",
      "2024-05-06 22:38:37,167 - INFO - Getting medical specialty for text: Caucasian male 22, height 5'11, 145lb/65.7kg \n",
      "\n",
      "If \n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "2024-05-06 22:38:41,500 - INFO - Extracted diseases: []\n",
      "2024-05-06 22:38:41,502 - INFO - Running zero-shot classification with hypothesis template\n",
      "2024-05-06 22:39:25,879 - INFO - Top medical specialty: Endocrinology\n",
      "2024-05-06 22:39:25,882 - INFO - Getting medical specialty for text: 12 months old\n",
      "\n",
      "Female\n",
      "\n",
      "No medications\n",
      "\n",
      "Labs\n",
      "\n",
      "Alkal\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "2024-05-06 22:39:29,503 - INFO - Extracted diseases: []\n",
      "2024-05-06 22:39:29,505 - INFO - Running zero-shot classification with hypothesis template\n",
      "2024-05-06 22:41:56,710 - INFO - Top medical specialty: Endocrinology\n",
      "2024-05-06 22:41:56,715 - INFO - Finding doctor advice comment in comments\n",
      "2024-05-06 22:41:56,718 - INFO - Checking comment: there is unlikely to be anything physically wrong \n",
      "2024-05-06 22:41:56,721 - INFO - Checking if comment contains doctor advice: there is unlikely to be anything physically wrong \n",
      "2024-05-06 22:41:56,723 - INFO - Running zero-shot classifier on comment\n",
      "2024-05-06 22:41:58,841 - INFO - Classifier result: true\n",
      "2024-05-06 22:41:58,842 - INFO - Found doctor advice comment: there is unlikely to be anything physically wrong \n",
      "2024-05-06 22:41:58,844 - INFO - Finding doctor advice comment in comments\n",
      "2024-05-06 22:41:58,845 - INFO - Checking comment: all of her bloods are relatively normal (even some\n",
      "2024-05-06 22:41:58,846 - INFO - Checking if comment contains doctor advice: all of her bloods are relatively normal (even some\n",
      "2024-05-06 22:41:58,849 - INFO - Running zero-shot classifier on comment\n",
      "2024-05-06 22:42:03,241 - INFO - Classifier result: true\n",
      "2024-05-06 22:42:03,243 - INFO - Found doctor advice comment: all of her bloods are relatively normal (even some\n",
      "Your max_length is set to 50, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Why do I get sick when I don't drink milk?. ca...\n",
      "1    12 month old unresponsive, stumped hospital. A...\n",
      "Name: question, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def extract_disease_names_hf(text, max_length=512):\n",
    "    # Load tokenizer and model from Hugging Face Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Clinical-AI-Apollo/Medical-NER\", max_length=max_length, truncation=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"Clinical-AI-Apollo/Medical-NER\")\n",
    "    \n",
    "    # Create a pipeline for named entity recognition\n",
    "    pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy='simple')\n",
    "    \n",
    "    # Split the text into smaller segments\n",
    "    segments = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "    \n",
    "    diseases = []\n",
    "    for segment in segments:\n",
    "        # Process each segment through the pipeline\n",
    "        ner_results = pipe(segment)\n",
    "        # Extract entities labeled as diseases (depending on the model's labeling scheme)\n",
    "        segment_diseases = [result['word'] for result in ner_results if 'disease' in result['entity_group'].lower()]\n",
    "        diseases.extend(segment_diseases)\n",
    "    \n",
    "    return diseases\n",
    "\n",
    "class MedicalSpecialtyPipeline:\n",
    "    def __init__(self, model_name=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\", max_length=512):\n",
    "        logging.info(\"Initializing MedicalSpecialtyPipeline with model: %s\", model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, max_length=max_length, truncation=True)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.medical_specialties = [\n",
    "                    \"Cardiology\",\n",
    "                    \"Dermatology\",\n",
    "                    \"Emergency Medicine\",\n",
    "                    \"Endocrinology\",\n",
    "                    \"Gastroenterology\",\n",
    "                    \"General Surgery\",\n",
    "                    \"Geriatrics\",\n",
    "                    \"Gynecology\",\n",
    "                    \"Hematology\",\n",
    "                    \"Infectious Disease\",\n",
    "                    \"Internal Medicine\",\n",
    "                    \"Nephrology\",\n",
    "                    \"Neurology\",\n",
    "                    \"Obstetrics\",\n",
    "                    \"Oncology\",\n",
    "                    \"Ophthalmology\",\n",
    "                    \"Orthopedics\",\n",
    "                    \"Otolaryngology (ENT)\",\n",
    "                    \"Pediatrics\",\n",
    "                    \"Psychiatry\",\n",
    "                    \"Pulmonology\",\n",
    "                    \"Rheumatology\",\n",
    "                    \"Urology\",\n",
    "                    \"Others\"\n",
    "        ]\n",
    "        self.advice_classifier = pipeline(\"zero-shot-classification\", model=model_name)\n",
    "        self.toxicity_classifier = pipeline(\"text-classification\", model=\"unitary/toxic-bert\")\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def get_medical_specialty(self, text):\n",
    "        logging.info(\"Getting medical specialty for text: %s\", text[:50])\n",
    "        \n",
    "        # Extract disease names from the text\n",
    "        diseases = extract_disease_names_hf(text, max_length=self.max_length)\n",
    "        logging.info(\"Extracted diseases: %s\", diseases)\n",
    "        \n",
    "        # Formulate the input for zero-shot classification\n",
    "        hypothesis_template = \"This medical case involves {}.\"\n",
    "        \n",
    "        # Classify medical specialty using zero-shot classification\n",
    "        logging.info(\"Running zero-shot classification with hypothesis template\")\n",
    "        result = self.advice_classifier(text, self.medical_specialties, hypothesis_template=hypothesis_template)\n",
    "        \n",
    "        # Extract top result\n",
    "        top_specialty = result['labels'][0]\n",
    "        logging.info(\"Top medical specialty: %s\", top_specialty)\n",
    "        \n",
    "        return top_specialty\n",
    "\n",
    "    def is_doctor_advice(self, comment):\n",
    "        logging.info(\"Checking if comment contains doctor advice: %s\", comment[:50])\n",
    "        # Adjust the hypothesis template to be more explicit and contextual\n",
    "        hypothesis_template = \"The statement '{}', is a piece of medical advice.\"\n",
    "        candidate_labels = [\"true\", \"false\"]  # Using true/false to align with the hypothesis\n",
    "        logging.info(\"Running zero-shot classifier on comment\")\n",
    "        # Adjust the call to pass the hypothesis template\n",
    "        result = self.advice_classifier(hypothesis_template.format(comment), candidate_labels)\n",
    "        logging.info(\"Classifier result: %s\", result['labels'][0])\n",
    "        return result['labels'][0] == 'true'\n",
    "\n",
    "    def find_doctor_advice_comment(self, comments):\n",
    "        logging.info(\"Finding doctor advice comment in comments\")\n",
    "        # Ensure comments are iterated correctly\n",
    "        if isinstance(comments, str):\n",
    "            comments = [comments]  # Single string to list\n",
    "        elif isinstance(comments, list):\n",
    "            pass  # Already in list form, do nothing\n",
    "        else:\n",
    "            logging.error(\"Unsupported comment format: %s\", type(comments))\n",
    "            return None\n",
    "\n",
    "        for comment in comments:\n",
    "            logging.info(\"Checking comment: %s\", comment[:50])\n",
    "            if self.is_doctor_advice(comment):\n",
    "                logging.info(\"Found doctor advice comment: %s\", comment[:50])\n",
    "                return comment\n",
    "        logging.info(\"No doctor advice comment found\")\n",
    "        return None\n",
    "    \n",
    "    def is_toxic(self, text):\n",
    "        result = self.toxicity_classifier(text[:self.max_length])[0]\n",
    "        return result['label'] == 'toxic' and result['score'] >= 0.7\n",
    "\n",
    "    def process_dataframe(self, df):\n",
    "        logging.info(\"Processing dataframe with %d rows\", len(df))\n",
    "        df = df.drop_duplicates(subset='id', keep='first')\n",
    "        df = df[df['num_comments']< 15]\n",
    "        df = df[['title','selftext','comments', 'flair']]\n",
    "        df = df.reset_index()\n",
    "        df.drop(['index'], inplace=True, axis=1)\n",
    "        df['comments'] = df['comments'].apply(ast.literal_eval)\n",
    "        df['medical_specialty'] = df['selftext'].apply(self.get_medical_specialty)\n",
    "        df['doctor_advice_comment'] = df['comments'].apply(self.find_doctor_advice_comment)\n",
    "        \n",
    "        # Toxicity analysis\n",
    "        df['title_non_toxic'] = ~df['title'].apply(self.is_toxic)\n",
    "        df['selftext_non_toxic'] = ~df['selftext'].apply(self.is_toxic)\n",
    "        df['comments_non_toxic'] = df['comments'].apply(lambda comments: all(~self.is_toxic(comment) for comment in comments))\n",
    "        \n",
    "        # Drop rows with toxic content\n",
    "        df = df[(df['title_non_toxic'] == True) & (df['selftext_non_toxic'] == True) & (df['comments_non_toxic'] == True)]\n",
    "        \n",
    "        return df\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "class MedicalSummaryPipeline:\n",
    "    def __init__(self, summarization_model_name=\"Falconsai/medical_summarization\", max_length=512):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(summarization_model_name, max_length=max_length, truncation=True)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(summarization_model_name)\n",
    "        self.summarizer = pipeline(\"summarization\", model=self.model, tokenizer=self.tokenizer, max_length=max_length)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def summarize_text(self, text, max_length=50, min_length=10, do_sample=False):\n",
    "        # Split the text into smaller segments\n",
    "        segments = [text[i:i+self.max_length] for i in range(0, len(text), self.max_length)]\n",
    "        \n",
    "        summaries = []\n",
    "        for segment in segments:\n",
    "            # Summarize each segment\n",
    "            summary = self.summarizer(segment, max_length=max_length, min_length=min_length, do_sample=do_sample)[0]['summary_text']\n",
    "            summaries.append(summary)\n",
    "        \n",
    "        # Combine the summaries\n",
    "        combined_summary = ' '.join(summaries)\n",
    "        \n",
    "        return combined_summary\n",
    "\n",
    "    def process_dataframe(self, df):\n",
    "        df['selftext_summary'] = df['selftext'].apply(self.summarize_text)\n",
    "        df['question'] = df['title'] + '. ' + df['selftext_summary']\n",
    "        return df\n",
    "    \n",
    "\n",
    "class PromptInstructionDataset:\n",
    "    def __init__(self, data_file):\n",
    "        self.data_file = data_file\n",
    "        self.df = pd.read_csv(data_file)\n",
    "\n",
    "    def create_dataset(self):\n",
    "        prompt_instruction_pairs = []\n",
    "\n",
    "        for index, row in self.df.iterrows():\n",
    "            question = row['question']\n",
    "            doctor_advice_comment = row['doctor_advice_comment']\n",
    "            medical_specialty = row['medical_specialty']\n",
    "\n",
    "            prompt = f\"Question: {question}\\n\\nBased on the above information, provide a general medical advice comment and suggest the most appropriate medical specialty.\"\n",
    "            instruction = f\"Specialty Suggestion: {medical_specialty}\\nMedical Advice: {doctor_advice_comment}\"\n",
    "\n",
    "            prompt_instruction_pairs.append({\"prompt\": prompt, \"instruction\": instruction})\n",
    "\n",
    "        prompt_instruction_df = pd.DataFrame(prompt_instruction_pairs)\n",
    "        return prompt_instruction_df\n",
    "\n",
    "    def save_dataset(self, output_file):\n",
    "        prompt_instruction_df = self.create_dataset()\n",
    "        prompt_instruction_df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "# Example usage and testing\n",
    "df = pd.read_csv('data.csv')\n",
    "df = df[:5]\n",
    "medical_pipeline = MedicalSpecialtyPipeline(max_length=512)\n",
    "df = medical_pipeline.process_dataframe(df)\n",
    "df.to_csv('pro.csv')\n",
    "\n",
    "summary_pipeline = MedicalSummaryPipeline(max_length=512)\n",
    "df = summary_pipeline.process_dataframe(df)\n",
    "print(df['question'].head())\n",
    "df = df[['question','doctor_advice_comment','medical_specialty']]\n",
    "\n",
    "df.to_csv('sum.csv')\n",
    "\n",
    "# prompt_dataset = PromptInstructionDataset('sum.csv')\n",
    "# prompt_dataset.save_dataset('prompt_instruction_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c1741-6c61-4471-ab70-1d1bd3ac90dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "115a5e08-5359-40fa-be4a-c60be234e562",
   "metadata": {},
   "source": [
    "## Data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7e6a3265-c4c0-43b3-bba0-815f00561435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b77f9ada-ac87-4928-a242-e30e02086410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72360ff7-0238-466e-a866-a82b931c3c76",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787cece6-8659-4dbc-a8ed-6002e1c84565",
   "metadata": {},
   "source": [
    "3 potential models selected, Phi3, llama 3 and mistral. Benchmarking below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b67da3e-7208-4007-b70d-11df0a895a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def medical_assistant_prompt(user_query):\n",
    "    prompt = f\"\"\"\n",
    "User Query: {user_query}\n",
    "[You are a medical professional bot that behaves like a doctor. You only answer to medical-related queries. If you encounter off-topics, you should respond with, 'As an AI medical assistant, my expertise lies in providing assistance with medical queries. Unfortunately, I'm unable to address non-medical topics. If you have any medical-related questions or concerns, feel free to ask, and I'll do my best to assist you']\n",
    "\n",
    "Medical Specialty Prediction:\n",
    "Based on the provided query, the most relevant medical specialty is likely [Insert predicted medical specialty]. This specialty deals with [Briefly describe the focus of the predicted medical specialty].\n",
    "\n",
    "Expert Response:\n",
    "[Provide a detailed, expert-level response to the user's query based on the predicted medical specialty, including the following:\n",
    "- Possible diagnosis or differential diagnoses\n",
    "- Brief explanation of the suspected condition(s)\n",
    "- Potential causes or risk factors\n",
    "- Recommended diagnostic tests or procedures\n",
    "- Treatment options and management strategies\n",
    "- Prognosis and potential complications\n",
    "- Guidance on when to seek further medical attention]\n",
    "\n",
    "Lifestyle Recommendations:\n",
    "[Insert specific advice on lifestyle modifications, such as diet, exercise, sleep, or stress management, that may help manage the condition or alleviate symptoms]\n",
    "\n",
    "Next Steps:\n",
    "[Insert recommendations for follow-up with a healthcare provider, including the type of specialist to consult and the timing of the follow-up]\n",
    "\n",
    "Disclaimer:\n",
    "This information is provided for educational purposes only and is not intended to replace professional medical advice, diagnosis, or treatment. Always seek the guidance of a qualified healthcare provider with any questions regarding a medical condition or treatment.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69c54335-fd94-472b-bfac-ad37d3bbf5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "user_query = \"I have been experiencing persistent headaches, sensitivity to light, and occasional nausea for the past few weeks. What could be causing these symptoms, and what should I do?\"\n",
    "\n",
    "prompt = medical_assistant_prompt(user_query)\n",
    "\n",
    "# Make a request to the Mistral API\n",
    "api_key = \"YOUR_API_KEY\"\n",
    "endpoint = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "   \n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"mistral\",\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 500,\n",
    "    \"temperature\": 0.7,\n",
    "    \"stream\":False\n",
    "}\n",
    "\n",
    "response = requests.post(endpoint, headers=headers, json=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    # generated_text = result[\"choices\"][0][\"text\"]\n",
    "    print(result)\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "98daaa18-67a1-40a1-b52c-c323611ea353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'created_at', 'response', 'done', 'context', 'total_duration', 'load_duration', 'prompt_eval_count', 'prompt_eval_duration', 'eval_count', 'eval_duration'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "18622f33-b237-4d52-a03c-c2e08353ded6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  Predicted Medical Specialty: Neurology\n",
      "\n",
      "Expert Response:\n",
      "Based on your symptoms of persistent headaches, sensitivity to light, and occasional nausea for several weeks, it is important that you consult with a neurologist. These symptoms could be indicative of various conditions, including migraines, tension headaches, or more serious disorders such as a brain tumor or a condition called photophobia (extreme sensitivity to light).\n",
      "\n",
      "Migraines are characterized by recurring headaches, usually unilateral, often accompanied by nausea, vomiting, and sensitivity to light and sound. Tension headaches are another common type of headache that may cause constant pain or pressure in the forehead, temples, or back of the head.\n",
      "\n",
      "The exact causes of these conditions vary. Migraines may be triggered by stress, certain foods, hormonal changes, or other factors. Tension headaches can result from muscle tension due to stress, poor posture, or other reasons. Brain tumors and photophobia are less common causes and require further investigation.\n",
      "\n",
      "To help determine the cause of your symptoms, a neurologist may recommend several diagnostic tests, including:\n",
      "- Physical examination: To evaluate your overall health and assess any neurological issues\n",
      "- Imaging studies: Such as MRI or CT scans to examine your brain structure and rule out conditions like brain tumors\n",
      "- Blood tests: To check for any underlying medical conditions or infections that could be causing your symptoms\n",
      "- Neurological assessments: Such as an electroencephalogram (EEG) or electromyography (EMG) to evaluate the function of your nerves and muscles\n",
      "\n",
      "Treatment options for these conditions depend on their underlying causes. For migraines, lifestyle modifications such as stress management, regular exercise, and a healthy diet can help alleviate symptoms. Prescription medications may also be recommended to prevent or relieve migraines. For tension headaches, similar lifestyle modifications, as well as over-the-counter pain relievers, can provide relief. In some cases, prescription medications or Botox injections may be necessary. Brain tumors and photophobia require more specialized treatment.\n",
      "\n",
      "The prognosis for these conditions varies. Migraines and tension headaches typically have good prognoses with proper management, while brain tumors and photophobia can potentially result in more serious complications if left untreated.\n",
      "\n",
      "Lifestyle Recommendations:\n",
      "To manage your symptoms, consider the following lifestyle modifications:\n",
      "- Practice stress reduction techniques, such as deep breathing exercises or meditation\n",
      "- Get regular exercise, aiming for at least 30 minutes per day, five days a week\n",
      "- Maintain a healthy diet, avoiding known triggers like caffeine, alcohol, and processed foods\n",
      "- Prioritize good sleep hygiene, ensuring you get 7-9 hours of quality sleep each night\n",
      "- Consider keeping a headache diary to help identify potential triggers\n",
      "\n",
      "Next Steps:\n",
      "If you have not yet consulted with a healthcare provider regarding your symptoms, I strongly recommend scheduling an appointment with a neurologist. If you already have a neurology consultation scheduled, be sure to bring a detailed list of your symptoms and any concerns you may have for the specialist to review during your visit. Remember that this information is meant for educational purposes only, and it's always crucial to consult with a qualified healthcare professional for medical advice. \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: {result['response']} \\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4f360bf-ab0b-4d35-a7a5-1b1dc27490f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'mistral', 'created_at': '2024-05-06T10:07:50.995069Z', 'response': \" As an assistant, I'd be happy to help answer any medical-related queries you might have. However, based on your query, it seems more suitable for an educational assistant rather than a medical professional. Since I am programmed as a medical professional bot, I cannot directly help you with your homework. I can, however, provide some general information related to various medical conditions and topics if you'd like.\\n\\nIf we were to predict a medical specialty based on the given context, it could be General Practice or Pediatrics, as these specialties deal with a wide range of health concerns. In response to your query, I can give you some general information about various common conditions that students might encounter during their homework assignments.\\n\\n1. Diabetes Mellitus: A chronic condition characterized by high blood sugar levels due to the body's inability to produce or effectively use insulin. Symptoms include increased thirst, frequent urination, fatigue, and blurred vision. Causes may be genetic or environmental factors. Treatment includes lifestyle modifications (diet, exercise), medications, and regular monitoring of blood sugar levels.\\n\\n2. Asthma: A chronic respiratory condition characterized by inflammation and narrowing of the airways, leading to difficulty breathing. Symptoms include coughing, wheezing, shortness of breath, and chest tightness. Causes may be genetic or environmental factors, such as allergens or irritants. Treatment includes medications (inhalers), lifestyle modifications (avoiding triggers), and regular check-ups with a healthcare provider.\\n\\n3. Anemia: A condition characterized by a lack of sufficient red blood cells to carry oxygen throughout the body. Symptoms include fatigue, weakness, shortness of breath, and pale skin. Causes may be due to nutritional deficiencies (iron-deficiency anemia), genetic conditions, or chronic diseases. Treatment includes addressing the underlying cause (nutritional supplements, medications, or blood transfusions) and managing symptoms with iron supplements, vitamins, and other supportive care.\\n\\n4. Influenza: A viral infection that affects the respiratory system, causing symptoms such as fever, coughing, sore throat, body aches, headache, and fatigue. Prevention measures include getting an annual flu shot, practicing good hand hygiene, and avoiding contact with sick individuals. Treatment includes rest, plenty of fluids, and over-the-counter pain relievers for symptoms.\\n\\nLifestyle recommendations: Regardless of the condition, maintaining a healthy lifestyle is crucial for overall wellbeing. This includes:\\n\\n- Eating a balanced diet rich in fruits, vegetables, lean proteins, and whole grains\\n- Getting regular physical activity (30 minutes most days of the week)\\n- Ensuring adequate sleep each night (7-9 hours recommended for adults)\\n- Managing stress through relaxation techniques, hobbies, or seeking support from friends, family, or mental health professionals\\n\\nNext steps: If you suspect that you or someone else has any of these conditions or symptoms, it's essential to consult a healthcare professional for proper diagnosis and treatment. They can provide a more accurate assessment and recommend the appropriate course of action based on your individual circumstances.\", 'done': True, 'context': [733, 16289, 28793, 259, 13, 730, 16580, 28747, 17162, 541, 368, 1316, 528, 511, 586, 26284, 4665, 28804, 13, 28792, 1976, 460, 264, 5714, 5024, 12435, 369, 1833, 3410, 737, 264, 6676, 28723, 995, 865, 4372, 298, 5714, 28733, 9646, 23681, 28723, 1047, 368, 10301, 805, 28733, 3746, 1063, 28725, 368, 1023, 9421, 395, 28725, 464, 2198, 396, 16107, 5714, 13892, 28725, 586, 14900, 10427, 297, 7501, 11611, 395, 5714, 23681, 28723, 13445, 28725, 315, 28742, 28719, 9638, 298, 2962, 1843, 28733, 1591, 745, 13817, 28723, 1047, 368, 506, 707, 5714, 28733, 9646, 4224, 442, 10864, 28725, 1601, 1933, 298, 1460, 28725, 304, 315, 28742, 584, 511, 586, 1489, 298, 6031, 368, 1421, 13, 13, 15774, 745, 8942, 884, 19122, 3033, 28747, 13, 24207, 356, 272, 3857, 5709, 28725, 272, 1080, 8598, 5714, 2841, 884, 349, 3917, 733, 13851, 17931, 5714, 2841, 884, 1592, 851, 2841, 884, 14685, 395, 733, 28760, 4667, 346, 6685, 272, 3232, 302, 272, 17931, 5714, 2841, 884, 1592, 13, 13, 966, 2482, 12107, 28747, 13, 28792, 18325, 547, 264, 10537, 28725, 7583, 28733, 4404, 2899, 298, 272, 2188, 28742, 28713, 5709, 2818, 356, 272, 17931, 5714, 2841, 884, 28725, 2490, 272, 2296, 28747, 13, 28733, 19796, 1070, 21967, 442, 21813, 12008, 14779, 13, 28733, 365, 4667, 13268, 302, 272, 18648, 4644, 28732, 28713, 28731, 13, 28733, 10650, 2256, 10110, 442, 4623, 8612, 13, 28733, 1298, 1805, 2508, 23360, 8079, 442, 15251, 13, 28733, 15424, 466, 2877, 304, 5411, 12108, 13, 28733, 1133, 4206, 9795, 304, 4628, 9768, 697, 13, 28733, 2480, 313, 617, 356, 739, 298, 5695, 3629, 5714, 4501, 28793, 13, 13, 28758, 7541, 1487, 1298, 1805, 416, 697, 28747, 13, 28792, 13851, 2948, 7478, 356, 16218, 26470, 28725, 1259, 390, 9751, 28725, 9095, 28725, 4289, 28725, 442, 6727, 5411, 28725, 369, 993, 1316, 8594, 272, 4644, 442, 389, 2303, 13713, 12380, 28793, 13, 13, 6693, 2349, 782, 28747, 13, 28792, 13851, 18892, 354, 1372, 28733, 715, 395, 264, 15240, 9782, 28725, 2490, 272, 1212, 302, 19899, 298, 7731, 304, 272, 18597, 302, 272, 1372, 28733, 715, 28793, 13, 13, 3278, 20032, 28747, 13, 3260, 1871, 349, 3857, 354, 14165, 10700, 865, 304, 349, 459, 8926, 298, 9013, 5024, 5714, 7478, 28725, 21967, 28725, 442, 5827, 28723, 17484, 5695, 272, 15988, 302, 264, 14786, 15240, 9782, 395, 707, 4224, 8217, 264, 5714, 4644, 442, 5827, 28723, 13, 13, 13, 733, 28748, 16289, 28793, 1136, 396, 13892, 28725, 315, 28742, 28715, 347, 4610, 298, 1316, 4372, 707, 5714, 28733, 9646, 23681, 368, 1659, 506, 28723, 2993, 28725, 2818, 356, 574, 5709, 28725, 378, 3969, 680, 11633, 354, 396, 14165, 13892, 3210, 821, 264, 5714, 5024, 28723, 4577, 315, 837, 2007, 1591, 390, 264, 5714, 5024, 12435, 28725, 315, 3573, 5090, 1316, 368, 395, 574, 26284, 28723, 315, 541, 28725, 3545, 28725, 3084, 741, 2952, 1871, 5202, 298, 4118, 5714, 4331, 304, 13817, 513, 368, 28742, 28715, 737, 28723, 13, 13, 3381, 478, 654, 298, 6782, 264, 5714, 2841, 884, 2818, 356, 272, 2078, 2758, 28725, 378, 829, 347, 3592, 25508, 442, 11943, 9348, 6720, 28725, 390, 1167, 2841, 22833, 3215, 395, 264, 5335, 2819, 302, 2528, 10864, 28723, 560, 2899, 298, 574, 5709, 28725, 315, 541, 2111, 368, 741, 2952, 1871, 684, 4118, 3298, 4331, 369, 3567, 1659, 10301, 1938, 652, 26284, 28025, 28723, 13, 13, 28740, 28723, 6216, 375, 10072, 351, 479, 279, 381, 28747, 330, 20388, 4644, 23100, 486, 1486, 4242, 9984, 6157, 2940, 298, 272, 2187, 28742, 28713, 297, 2437, 298, 7072, 442, 11466, 938, 1488, 24726, 28723, 12356, 447, 5185, 3024, 7483, 306, 777, 28725, 16153, 4273, 2235, 28725, 6370, 12216, 28725, 304, 19690, 893, 8021, 28723, 334, 23829, 993, 347, 19869, 442, 12507, 8612, 28723, 15424, 466, 5532, 16218, 26470, 325, 28715, 1296, 28725, 9095, 557, 28050, 28725, 304, 4392, 16882, 302, 4242, 9984, 6157, 28723, 13, 13, 28750, 28723, 15091, 28716, 705, 28747, 330, 20388, 10840, 361, 5377, 4644, 23100, 486, 3661, 6461, 352, 304, 8484, 288, 302, 272, 2423, 1504, 28725, 5374, 298, 14426, 14232, 28723, 12356, 447, 5185, 3024, 26245, 288, 28725, 2277, 5620, 288, 28725, 2485, 1467, 302, 5276, 28725, 304, 8118, 6975, 1467, 28723, 334, 23829, 993, 347, 19869, 442, 12507, 8612, 28725, 1259, 390, 27451, 596, 442, 16418, 1549, 28723, 15424, 466, 5532, 28050, 325, 262, 6019, 404, 557, 16218, 26470, 325, 494, 806, 288, 467, 21102, 557, 304, 4392, 1877, 28733, 9023, 395, 264, 15240, 9782, 28723, 13, 13, 28770, 28723, 1094, 366, 515, 28747, 330, 4644, 23100, 486, 264, 5502, 302, 9406, 2760, 4242, 8894, 298, 7096, 21058, 5473, 272, 2187, 28723, 12356, 447, 5185, 3024, 6370, 12216, 28725, 18288, 28725, 2485, 1467, 302, 5276, 28725, 304, 12805, 4759, 28723, 334, 23829, 993, 347, 2940, 298, 9246, 28712, 2582, 801, 2650, 6094, 325, 2311, 28733, 1270, 9375, 396, 366, 515, 557, 19869, 4331, 28725, 442, 20388, 18257, 28723, 15424, 466, 5532, 24643, 272, 14164, 4244, 325, 17964, 28712, 2582, 12982, 28713, 28725, 28050, 28725, 442, 4242, 1203, 28722, 12378, 28731, 304, 16097, 12380, 395, 8075, 12982, 28713, 28725, 11781, 314, 1126, 28725, 304, 799, 27729, 1656, 28723, 13, 13, 28781, 28723, 560, 1134, 8256, 2166, 28747, 330, 7692, 282, 18197, 369, 20777, 272, 10840, 361, 5377, 1587, 28725, 13098, 12380, 1259, 390, 22041, 28725, 26245, 288, 28725, 23176, 10807, 28725, 2187, 264, 1927, 28725, 1335, 1344, 28725, 304, 6370, 12216, 28723, 4258, 6254, 10582, 3024, 2719, 396, 9558, 13857, 5180, 28725, 28172, 1179, 1021, 6521, 28721, 6863, 28725, 304, 23329, 3754, 395, 7946, 6992, 28723, 15424, 466, 5532, 1846, 28725, 8969, 302, 13857, 2298, 28725, 304, 754, 28733, 1237, 28733, 12520, 3358, 14267, 740, 354, 12380, 28723, 13, 13, 28758, 7541, 1487, 18892, 28747, 2678, 26343, 302, 272, 4644, 28725, 17877, 264, 7783, 16218, 349, 13040, 354, 7544, 1162, 15823, 28723, 851, 5532, 28747, 13, 13, 28733, 413, 1077, 264, 19971, 9751, 6708, 297, 21566, 28725, 19045, 28725, 6115, 2096, 1126, 28725, 304, 2894, 847, 1606, 13, 28733, 19896, 4392, 5277, 6355, 325, 28770, 28734, 3486, 1080, 2202, 302, 272, 1819, 28731, 13, 28733, 1618, 28713, 1378, 23193, 4289, 1430, 2125, 325, 28787, 28733, 28774, 3316, 11572, 354, 12111, 28731, 13, 28733, 2213, 4054, 6727, 1059, 27607, 9804, 28725, 295, 17787, 497, 28725, 442, 11246, 1760, 477, 3282, 28725, 2005, 28725, 442, 8057, 2528, 12749, 13, 13, 6693, 5944, 28747, 1047, 368, 12236, 369, 368, 442, 2493, 1112, 659, 707, 302, 1167, 4331, 442, 12380, 28725, 378, 28742, 28713, 7974, 298, 7731, 264, 15240, 5024, 354, 4979, 21967, 304, 5827, 28723, 1306, 541, 3084, 264, 680, 11229, 15081, 304, 6557, 272, 7658, 2363, 302, 2992, 2818, 356, 574, 3235, 10139, 28723], 'total_duration': 184477053900, 'load_duration': 13028861500, 'prompt_eval_count': 400, 'prompt_eval_duration': 1075911000, 'eval_count': 701, 'eval_duration': 170370431000}\n"
     ]
    }
   ],
   "source": [
    "user_query2 = \"Hey can you help me do my homework please?\"\n",
    "prompt2 = medical_assistant_prompt(user_query2)\n",
    "response_2 = requests.post(endpoint, headers = headers, json = {\"model\":\"mistral\", \"prompt\":prompt2,\"max_tokens\":80, \"temperature\": 0.7, \"stream\": False})\n",
    "\n",
    "if response_2.status_code == 200:\n",
    "    result = response_2.json()\n",
    "    # generated_text = result[\"choices\"][0][\"text\"]\n",
    "    print(result)\n",
    "else:\n",
    "    print(f\"Error: {response_2.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "464aab52-f193-4aa8-9684-94db8c3df2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_call(user_query):\n",
    "    response = requests.post(endpoint, headers = headers, json = {\"model\":\"mistral\", \"prompt\":prompt2,\"max_tokens\":80, \"temperature\": 0.7, \"stream\": False})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = reponse.json()\n",
    "        # generated_text = result[\"choices\"][0][\"text\"]\n",
    "        print(result)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da799d8-e8ed-499a-8d83-bb206f2fec16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
