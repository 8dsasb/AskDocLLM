{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cfffbfae-115a-4b87-b269-8643c598a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import praw\n",
    "import html\n",
    "import ast\n",
    "import re\n",
    "import ftfy\n",
    "from markdown_it import MarkdownIt\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda34cfe-98c1-4869-8dc2-7f36113ca07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = \"AskDoc Scraper\"\n",
    "posts_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44c03b42-f495-48f7-82bb-2c4a2b16f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = praw.Reddit(\n",
    "    client_id = \"_JTrgh_onvOkRlXXuYk7xQ\",\n",
    "    client_secret = \"yjEhrCabnwnNb8gxXaQGL17Cn2Nsag\",\n",
    "    user_agent = user_agent\n",
    ")\n",
    "subreddit = rd.subreddit('AskDocs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d77a48-bb86-4fa8-88ba-27576105a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(sort_type = \"hot\", limit = 1000):\n",
    "    try:\n",
    "        if sort_type not in ['hot', 'top']:\n",
    "            raise ValueError(\"sort_type must be 'hot' or 'top'\")\n",
    "        posts_sort = subreddit.top(limit = limit) if sort_type == 'top' else subreddit.hot(limit = limit)\n",
    "        for index, post in enumerate(posts_sort):\n",
    "            if index % 10 == 0:\n",
    "                print(f\"Accessed {index} posts\") \n",
    "                \n",
    "            post_data = {\n",
    "                'id': post.id,\n",
    "                'title': post.title,\n",
    "                'selftext': post.selftext,\n",
    "                'author': post.author.name if post.author else None,\n",
    "                'score': post.score,\n",
    "                'num_comments': post.num_comments,\n",
    "                'created_utc': post.created_utc,\n",
    "                'flair': post.link_flair_text\n",
    "            }\n",
    "            \n",
    "            comments_data = []\n",
    "            post.comments.replace_more(limit=0)\n",
    "            for comment in post.comments.list():\n",
    "                comment_data = {\n",
    "                    'id': comment.id,\n",
    "                    'body': comment.body,\n",
    "                    'author': comment.author.name if comment.author else None,\n",
    "                    'score': comment.score,\n",
    "                    'parent_id': comment.parent_id,\n",
    "                    'created_utc': comment.created_utc,\n",
    "                    'is_submitter': comment.is_submitter\n",
    "                }\n",
    "                comments_data.append(comment_data)\n",
    "            \n",
    "            post_data['comments'] = comments_data\n",
    "            posts_data.append(post_data) \n",
    "            \n",
    "            if index % 100 == 0 and index != 0:  \n",
    "                initial_df = pd.DataFrame(posts_data)\n",
    "                initial_df.to_csv(f'reddit_data_{index}.csv', index=False)\n",
    "                print(f\"Data saved at {index} posts\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        initial_df = pd.DataFrame(posts_data)\n",
    "        initial_df.to_csv('reddit_data_error_save.csv', index=False)\n",
    "        raise\n",
    "    \n",
    "    posts_df = pd.DataFrame(posts_data)\n",
    "    posts_df.to_csv(f'AskDoc_{sort_type}data.csv', index = False)\n",
    "    print(\"Succesfully scraped the data\")\n",
    "    return posts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efcd8209-1403-49ce-b0f1-adc182514018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessed 0 posts\n",
      "Accessed 10 posts\n",
      "Accessed 20 posts\n",
      "Accessed 30 posts\n",
      "Accessed 40 posts\n",
      "Accessed 50 posts\n",
      "Accessed 60 posts\n",
      "Accessed 70 posts\n",
      "Accessed 80 posts\n",
      "Accessed 90 posts\n",
      "Accessed 100 posts\n",
      "Data saved at 100 posts\n",
      "Accessed 110 posts\n",
      "Accessed 120 posts\n",
      "Accessed 130 posts\n",
      "Accessed 140 posts\n",
      "Accessed 150 posts\n",
      "Accessed 160 posts\n",
      "Accessed 170 posts\n",
      "Accessed 180 posts\n",
      "Accessed 190 posts\n",
      "Accessed 200 posts\n",
      "Data saved at 200 posts\n",
      "Accessed 210 posts\n",
      "Accessed 220 posts\n",
      "Accessed 230 posts\n",
      "Accessed 240 posts\n",
      "Accessed 250 posts\n",
      "Accessed 260 posts\n",
      "Accessed 270 posts\n",
      "Accessed 280 posts\n",
      "Accessed 290 posts\n",
      "Accessed 300 posts\n",
      "Data saved at 300 posts\n",
      "Accessed 310 posts\n",
      "Accessed 320 posts\n",
      "Accessed 330 posts\n",
      "Accessed 340 posts\n",
      "Accessed 350 posts\n",
      "Accessed 360 posts\n",
      "Accessed 370 posts\n",
      "Accessed 380 posts\n",
      "Accessed 390 posts\n",
      "Accessed 400 posts\n",
      "Data saved at 400 posts\n",
      "Accessed 410 posts\n",
      "Accessed 420 posts\n",
      "Accessed 430 posts\n",
      "Accessed 440 posts\n",
      "Accessed 450 posts\n",
      "Accessed 460 posts\n",
      "Accessed 470 posts\n",
      "Accessed 480 posts\n",
      "Accessed 490 posts\n",
      "Accessed 500 posts\n",
      "Data saved at 500 posts\n",
      "Accessed 510 posts\n",
      "Accessed 520 posts\n",
      "Accessed 530 posts\n",
      "Accessed 540 posts\n",
      "Accessed 550 posts\n",
      "Accessed 560 posts\n",
      "Accessed 570 posts\n",
      "Accessed 580 posts\n",
      "Accessed 590 posts\n",
      "Accessed 600 posts\n",
      "Data saved at 600 posts\n",
      "Accessed 610 posts\n",
      "Accessed 620 posts\n",
      "Accessed 630 posts\n",
      "Accessed 640 posts\n",
      "Accessed 650 posts\n",
      "Accessed 660 posts\n",
      "Accessed 670 posts\n",
      "Accessed 680 posts\n",
      "Accessed 690 posts\n",
      "Accessed 700 posts\n",
      "Data saved at 700 posts\n",
      "Accessed 710 posts\n",
      "Accessed 720 posts\n",
      "Accessed 730 posts\n",
      "Accessed 740 posts\n",
      "Accessed 750 posts\n",
      "Accessed 760 posts\n",
      "Accessed 770 posts\n",
      "Accessed 780 posts\n",
      "Accessed 790 posts\n",
      "Accessed 800 posts\n",
      "Data saved at 800 posts\n",
      "Accessed 810 posts\n",
      "Accessed 820 posts\n",
      "Accessed 830 posts\n",
      "Accessed 840 posts\n",
      "Accessed 850 posts\n",
      "Accessed 860 posts\n",
      "Accessed 870 posts\n",
      "Accessed 880 posts\n",
      "Accessed 890 posts\n",
      "Accessed 900 posts\n",
      "Data saved at 900 posts\n",
      "Accessed 910 posts\n",
      "Accessed 920 posts\n",
      "Accessed 930 posts\n",
      "Accessed 940 posts\n",
      "Accessed 950 posts\n",
      "Accessed 960 posts\n",
      "Accessed 970 posts\n",
      "Accessed 980 posts\n",
      "Accessed 990 posts\n",
      "Succesfully scraped the data\n"
     ]
    }
   ],
   "source": [
    "top_posts = scrape_data(\"top\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc79f418-8e91-4d06-bca0-dbd028b4b2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessed 0 posts\n",
      "Accessed 10 posts\n",
      "Accessed 20 posts\n",
      "Accessed 30 posts\n",
      "Accessed 40 posts\n",
      "Accessed 50 posts\n",
      "Accessed 60 posts\n",
      "Accessed 70 posts\n",
      "Accessed 80 posts\n",
      "Accessed 90 posts\n",
      "Accessed 100 posts\n",
      "Data saved at 100 posts\n",
      "Accessed 110 posts\n",
      "Accessed 120 posts\n",
      "Accessed 130 posts\n",
      "Accessed 140 posts\n",
      "Accessed 150 posts\n",
      "Accessed 160 posts\n",
      "Accessed 170 posts\n",
      "Accessed 180 posts\n",
      "Accessed 190 posts\n",
      "Accessed 200 posts\n",
      "Data saved at 200 posts\n",
      "Accessed 210 posts\n",
      "Accessed 220 posts\n",
      "Accessed 230 posts\n",
      "Accessed 240 posts\n",
      "Accessed 250 posts\n",
      "Accessed 260 posts\n",
      "Accessed 270 posts\n",
      "Accessed 280 posts\n",
      "Accessed 290 posts\n",
      "Accessed 300 posts\n",
      "Data saved at 300 posts\n",
      "Accessed 310 posts\n",
      "Accessed 320 posts\n",
      "Accessed 330 posts\n",
      "Accessed 340 posts\n",
      "Accessed 350 posts\n",
      "Accessed 360 posts\n",
      "Accessed 370 posts\n",
      "Accessed 380 posts\n",
      "Accessed 390 posts\n",
      "Accessed 400 posts\n",
      "Data saved at 400 posts\n",
      "Accessed 410 posts\n",
      "Accessed 420 posts\n",
      "Accessed 430 posts\n",
      "Accessed 440 posts\n",
      "Accessed 450 posts\n",
      "Accessed 460 posts\n",
      "Accessed 470 posts\n",
      "Accessed 480 posts\n",
      "Accessed 490 posts\n",
      "Accessed 500 posts\n",
      "Data saved at 500 posts\n",
      "Accessed 510 posts\n",
      "Accessed 520 posts\n",
      "Accessed 530 posts\n",
      "Accessed 540 posts\n",
      "Accessed 550 posts\n",
      "Accessed 560 posts\n",
      "Accessed 570 posts\n",
      "Accessed 580 posts\n",
      "Accessed 590 posts\n",
      "Accessed 600 posts\n",
      "Data saved at 600 posts\n",
      "Accessed 610 posts\n",
      "Accessed 620 posts\n",
      "Accessed 630 posts\n",
      "Accessed 640 posts\n",
      "Accessed 650 posts\n",
      "Accessed 660 posts\n",
      "Accessed 670 posts\n",
      "Accessed 680 posts\n",
      "Accessed 690 posts\n",
      "Accessed 700 posts\n",
      "Data saved at 700 posts\n",
      "Accessed 710 posts\n",
      "Accessed 720 posts\n",
      "Accessed 730 posts\n",
      "Succesfully scraped the data\n"
     ]
    }
   ],
   "source": [
    "hot_posts = scrape_data(\"hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7c7fa5e-5c95-4010-a230-ee12634a874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('AskDoc_hot_data1.csv')\n",
    "# data1_copy = data1.copy()\n",
    "data2 = pd.read_csv('AskDoc_hot_data2.csv')\n",
    "# data2_copy = data2.copy()\n",
    "data3 = pd.read_csv('AskDoc_topdata.csv')\n",
    "# data3_copy = data3.copy()\n",
    "\n",
    "merged_data = pd.concat([data1, data2, data3], ignore_index = True)\n",
    "merged_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "final_data = merged_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf960ff-e716-4d50-a725-60c8dbd40edb",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Notes for later:\n",
    "\n",
    "handle null data\n",
    "\n",
    "handle duplicates\n",
    "\n",
    "Removing mod comments\n",
    "\n",
    "Since hot posts may have unmoderated comment, can't just rely on reddit rules to assume the data's good. Thus, gotta use all sorts of methods to clean the posts and comments before fine tuning the llm. My plan:\n",
    "\n",
    "replace special characters\n",
    "\n",
    "replace html stuff\n",
    "\n",
    "replace encoded characters\n",
    "\n",
    "detect non english language and exclude them\n",
    "\n",
    "spell check\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d78e4535-af9b-462f-b96e-9126f5bc6f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of rows for the scraped data: 6956\n"
     ]
    }
   ],
   "source": [
    "#null data handling\n",
    "print(f\"Initial number of rows for the scraped data: {len(final_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25047614-20b1-4872-a86d-b1e516a59d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdi = MarkdownIt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d662a397-089f-4b9d-857c-1bab1111e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, author):\n",
    "    # Skip processing for bot authors or deleted/removed content\n",
    "    if author in ('AskDocs-ModTeam', 'AutoModerator') or text in ('[removed]', '[deleted]'):\n",
    "        return None \n",
    "\n",
    "    emojis = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    \n",
    "    # Continue processing if not by a bot or deleted/removed\n",
    "    text = mdi.render(text)\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = html.unescape(text)\n",
    "    text = text.lower()  # Normalize case\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = emojis.sub(r'', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "final_data['title'] = final_data.apply(lambda row: clean_text(row['title'], row['author']), axis=1)\n",
    "final_data['selftext'] = final_data.apply(lambda row: clean_text(row['selftext'], row['author']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42c59fb5-cc86-4a57-b69e-c389a658c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comments(comments, author):\n",
    "    comments_list = ast.literal_eval(comments)\n",
    "    cleaned_comments = []\n",
    "    for comment in comments_list:\n",
    "        if comment['author'] not in ('AskDocs-ModTeam', 'AutoModerator') and comment['body'] not in ('[removed]', '[deleted]'):\n",
    "            cleaned_text = clean_text(comment['body'], comment['author'])\n",
    "            if cleaned_text is not None:\n",
    "                cleaned_comments.append(cleaned_text)\n",
    "    return cleaned_comments\n",
    "\n",
    "final_data['comments'] = final_data.apply(lambda row: clean_comments(row['comments'], row['author']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52b985d7-c01f-4228-a80d-80a3d799bc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after dropping null values: 4777\n"
     ]
    }
   ],
   "source": [
    "new_data = final_data.dropna()\n",
    "# final_data = final_data[final_data['comments'].str.len() > 5]\n",
    "print(f\"Number of rows after dropping null values: {len(new_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b82acaf0-0b03-437f-af31-b497001772f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data.to_csv('CleanData.csv', encoding = 'utf-8', index = False)\n",
    "# new_data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b24c8c89-fdfa-4d46-bae5-ac260c2fbd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry this is happening you dont say your moms age but its unusual to develop schizophrenia later in life there are many other things that can cause auditory and visual hallucinations that arent schizophrenia some of which are more likely than schizophrenia depending on her age although having a sibling with it does increase her risk alcohol use disorder can predispose someone to certain types of brain dysfunction and dementia that could look like this too its also worth noting that a ct scan cannot rule in or out schizophrenia its hard to speculate further based on the information you have\n"
     ]
    }
   ],
   "source": [
    "# new_data = pd.read_csv('CleanData.csv')\n",
    "# new_data.head(3)\n",
    "for x in new_data['comments']:\n",
    "    print(x[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47200250-1faa-42b7-be3f-f5dc998de570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Initialize the WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    #Stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Lemmatize tokens and remove stopwords and non-alphabetic tokens\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(token) for token in tokens if token.isalpha()\n",
    "    ]\n",
    "    \n",
    "    # Re-join lemmatized tokens into a single string\n",
    "    return ' '.join(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83f06e57-64e8-4cd5-a7f5-e9a78f397623",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_data = new_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b37f2fee-761a-48be-93bc-3c01b8de2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_data['title'] = stand_data['title'].apply(standardize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2229baf3-1c8a-4f63-9d5f-1cc32de51901",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_data['selftext'] = stand_data['selftext'].apply(standardize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8737ef08-b3c4-46f2-91f2-0b92b84d5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_data['comments'] = stand_data['comments'].apply(lambda x: [standardize_text(comment) for comment in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6b180c33-7d3c-467f-8c71-1f09624cd4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mother hospitalized with acute psychosis is it...</td>\n",
       "      <td>i wa informed that my mother and best friend w...</td>\n",
       "      <td>60</td>\n",
       "      <td>1.713978e+09</td>\n",
       "      <td>[sorry this is happening you dont say your mom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what are the actual chance of this happening</td>\n",
       "      <td>my son wa stillborn last month when i wa week ...</td>\n",
       "      <td>425</td>\n",
       "      <td>1.713915e+09</td>\n",
       "      <td>[sorry for your loss my deepest sympathy true ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is it weird or unsafe to clean your butthole i...</td>\n",
       "      <td>age sex male height weight race hispanic durat...</td>\n",
       "      <td>228</td>\n",
       "      <td>1.713921e+09</td>\n",
       "      <td>[short answer this is kinda strange but youre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>seeking adviceavenues to explore leg paralysis...</td>\n",
       "      <td>month ago i woke up with extreme leg pain to t...</td>\n",
       "      <td>9</td>\n",
       "      <td>1.713980e+09</td>\n",
       "      <td>[usual disclaimer no one can provide specific ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ultrasound report</td>\n",
       "      <td>male lump on right testicle complain about inc...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.713994e+09</td>\n",
       "      <td>[hello my friend im wary of firing off anythin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>light green pee advice pls</td>\n",
       "      <td>i am a year old female i workout and i smoke c...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.713994e+09</td>\n",
       "      <td>[hello there this is from healthlinecom blue o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>is anal sex safe for u</td>\n",
       "      <td>i and wife are interested in exploring anal pl...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.713998e+09</td>\n",
       "      <td>[there a risk of her prolapse returning probab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mastoiditis or just ingrown hair</td>\n",
       "      <td>my boyfriend is sick and he is male and ha flu...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.713990e+09</td>\n",
       "      <td>[mastoiditis until proven otherwise that look ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>is this something my doctor shouldve told me a...</td>\n",
       "      <td>foot inch pound after reading through the resu...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.713996e+09</td>\n",
       "      <td>[i dont think you should be mad about this the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>what else can i do</td>\n",
       "      <td>im year old male no med or smoking about pound...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.713994e+09</td>\n",
       "      <td>[consider hydrocolloid patch, day before, , , ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "1   mother hospitalized with acute psychosis is it...   \n",
       "3        what are the actual chance of this happening   \n",
       "4   is it weird or unsafe to clean your butthole i...   \n",
       "5   seeking adviceavenues to explore leg paralysis...   \n",
       "7                                   ultrasound report   \n",
       "8                          light green pee advice pls   \n",
       "16                             is anal sex safe for u   \n",
       "17                   mastoiditis or just ingrown hair   \n",
       "19  is this something my doctor shouldve told me a...   \n",
       "23                                 what else can i do   \n",
       "\n",
       "                                             selftext  score   created_utc  \\\n",
       "1   i wa informed that my mother and best friend w...     60  1.713978e+09   \n",
       "3   my son wa stillborn last month when i wa week ...    425  1.713915e+09   \n",
       "4   age sex male height weight race hispanic durat...    228  1.713921e+09   \n",
       "5   month ago i woke up with extreme leg pain to t...      9  1.713980e+09   \n",
       "7   male lump on right testicle complain about inc...      5  1.713994e+09   \n",
       "8   i am a year old female i workout and i smoke c...      3  1.713994e+09   \n",
       "16  i and wife are interested in exploring anal pl...      2  1.713998e+09   \n",
       "17  my boyfriend is sick and he is male and ha flu...      3  1.713990e+09   \n",
       "19  foot inch pound after reading through the resu...      2  1.713996e+09   \n",
       "23  im year old male no med or smoking about pound...      2  1.713994e+09   \n",
       "\n",
       "                                             comments  \n",
       "1   [sorry this is happening you dont say your mom...  \n",
       "3   [sorry for your loss my deepest sympathy true ...  \n",
       "4   [short answer this is kinda strange but youre ...  \n",
       "5   [usual disclaimer no one can provide specific ...  \n",
       "7   [hello my friend im wary of firing off anythin...  \n",
       "8   [hello there this is from healthlinecom blue o...  \n",
       "16  [there a risk of her prolapse returning probab...  \n",
       "17  [mastoiditis until proven otherwise that look ...  \n",
       "19  [i dont think you should be mad about this the...  \n",
       "23  [consider hydrocolloid patch, day before, , , ...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_data.drop(columns = ['author', 'num_comments', 'flair', 'id'])\n",
    "stand_data.drop(columns = ['author', 'num_comments', 'flair', 'id'], inplace = True)\n",
    "# stand_data.to_csv('StandardizedData.csv')\n",
    "\n",
    "# stand_data['comment'] = stand_data['comments'].apply(lambda x: x[0] if x else '')\n",
    "stand_data.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc04f2-234f-4226-b6df-4b7e2d4c8ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a261325b-d095-489a-b4ab-d7b0b6921cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "115a5e08-5359-40fa-be4a-c60be234e562",
   "metadata": {},
   "source": [
    "## Data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7e6a3265-c4c0-43b3-bba0-815f00561435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentiment_labels(text):\n",
    "#     sentiment = TextBlob(text).sentiment\n",
    "#     return 'Urgent' if sentiment.polarity < -0.3 else 'Non-Urgent'\n",
    "\n",
    "# # Example usage on selftext\n",
    "# stand_data['urgency'] = stand_data['selftext'].apply(sentiment_labels)\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b77f9ada-ac87-4928-a242-e30e02086410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b449a-25c8-4e32-878b-f9e0a4167193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_label(row):\n",
    "    # Prioritize urgent sentiment\n",
    "    if row['urgency'] == 'Urgent':\n",
    "        return 'Urgent Medical Advice Needed'\n",
    "    # Check for common medical conditions and symptoms\n",
    "    if any(term in ['fever', 'cough', 'pain'] for term in row['medical_terms']):\n",
    "        return 'Specific Medical Condition'\n",
    "    return 'General Medical Inquiry'\n",
    "\n",
    "df['label'] = df.apply(assign_label, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
