{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "cfffbfae-115a-4b87-b269-8643c598a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import praw\n",
    "import html\n",
    "import ast\n",
    "import re\n",
    "import ftfy\n",
    "from markdown_it import MarkdownIt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda34cfe-98c1-4869-8dc2-7f36113ca07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = \"AskDoc Scraper\"\n",
    "posts_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44c03b42-f495-48f7-82bb-2c4a2b16f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = praw.Reddit(\n",
    "    client_id = \"_JTrgh_onvOkRlXXuYk7xQ\",\n",
    "    client_secret = \"yjEhrCabnwnNb8gxXaQGL17Cn2Nsag\",\n",
    "    user_agent = user_agent\n",
    ")\n",
    "subreddit = rd.subreddit('AskDocs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d77a48-bb86-4fa8-88ba-27576105a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(sort_type = \"hot\", limit = 1000):\n",
    "    try:\n",
    "        if sort_type not in ['hot', 'top']:\n",
    "            raise ValueError(\"sort_type must be 'hot' or 'top'\")\n",
    "        posts_sort = subreddit.top(limit = limit) if sort_type == 'top' else subreddit.hot(limit = limit)\n",
    "        for index, post in enumerate(posts_sort):\n",
    "            if index % 10 == 0:\n",
    "                print(f\"Accessed {index} posts\") \n",
    "                \n",
    "            post_data = {\n",
    "                'id': post.id,\n",
    "                'title': post.title,\n",
    "                'selftext': post.selftext,\n",
    "                'author': post.author.name if post.author else None,\n",
    "                'score': post.score,\n",
    "                'num_comments': post.num_comments,\n",
    "                'created_utc': post.created_utc,\n",
    "                'flair': post.link_flair_text\n",
    "            }\n",
    "            \n",
    "            comments_data = []\n",
    "            post.comments.replace_more(limit=0)\n",
    "            for comment in post.comments.list():\n",
    "                comment_data = {\n",
    "                    'id': comment.id,\n",
    "                    'body': comment.body,\n",
    "                    'author': comment.author.name if comment.author else None,\n",
    "                    'score': comment.score,\n",
    "                    'parent_id': comment.parent_id,\n",
    "                    'created_utc': comment.created_utc,\n",
    "                    'is_submitter': comment.is_submitter\n",
    "                }\n",
    "                comments_data.append(comment_data)\n",
    "            \n",
    "            post_data['comments'] = comments_data\n",
    "            posts_data.append(post_data) \n",
    "            \n",
    "            if index % 100 == 0 and index != 0:  \n",
    "                initial_df = pd.DataFrame(posts_data)\n",
    "                initial_df.to_csv(f'reddit_data_{index}.csv', index=False)\n",
    "                print(f\"Data saved at {index} posts\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        initial_df = pd.DataFrame(posts_data)\n",
    "        initial_df.to_csv('reddit_data_error_save.csv', index=False)\n",
    "        raise\n",
    "    \n",
    "    posts_df = pd.DataFrame(posts_data)\n",
    "    posts_df.to_csv(f'AskDoc_{sort_type}data.csv', index = False)\n",
    "    print(\"Succesfully scraped the data\")\n",
    "    return posts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efcd8209-1403-49ce-b0f1-adc182514018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessed 0 posts\n",
      "Accessed 10 posts\n",
      "Accessed 20 posts\n",
      "Accessed 30 posts\n",
      "Accessed 40 posts\n",
      "Accessed 50 posts\n",
      "Accessed 60 posts\n",
      "Accessed 70 posts\n",
      "Accessed 80 posts\n",
      "Accessed 90 posts\n",
      "Accessed 100 posts\n",
      "Data saved at 100 posts\n",
      "Accessed 110 posts\n",
      "Accessed 120 posts\n",
      "Accessed 130 posts\n",
      "Accessed 140 posts\n",
      "Accessed 150 posts\n",
      "Accessed 160 posts\n",
      "Accessed 170 posts\n",
      "Accessed 180 posts\n",
      "Accessed 190 posts\n",
      "Accessed 200 posts\n",
      "Data saved at 200 posts\n",
      "Accessed 210 posts\n",
      "Accessed 220 posts\n",
      "Accessed 230 posts\n",
      "Accessed 240 posts\n",
      "Accessed 250 posts\n",
      "Accessed 260 posts\n",
      "Accessed 270 posts\n",
      "Accessed 280 posts\n",
      "Accessed 290 posts\n",
      "Accessed 300 posts\n",
      "Data saved at 300 posts\n",
      "Accessed 310 posts\n",
      "Accessed 320 posts\n",
      "Accessed 330 posts\n",
      "Accessed 340 posts\n",
      "Accessed 350 posts\n",
      "Accessed 360 posts\n",
      "Accessed 370 posts\n",
      "Accessed 380 posts\n",
      "Accessed 390 posts\n",
      "Accessed 400 posts\n",
      "Data saved at 400 posts\n",
      "Accessed 410 posts\n",
      "Accessed 420 posts\n",
      "Accessed 430 posts\n",
      "Accessed 440 posts\n",
      "Accessed 450 posts\n",
      "Accessed 460 posts\n",
      "Accessed 470 posts\n",
      "Accessed 480 posts\n",
      "Accessed 490 posts\n",
      "Accessed 500 posts\n",
      "Data saved at 500 posts\n",
      "Accessed 510 posts\n",
      "Accessed 520 posts\n",
      "Accessed 530 posts\n",
      "Accessed 540 posts\n",
      "Accessed 550 posts\n",
      "Accessed 560 posts\n",
      "Accessed 570 posts\n",
      "Accessed 580 posts\n",
      "Accessed 590 posts\n",
      "Accessed 600 posts\n",
      "Data saved at 600 posts\n",
      "Accessed 610 posts\n",
      "Accessed 620 posts\n",
      "Accessed 630 posts\n",
      "Accessed 640 posts\n",
      "Accessed 650 posts\n",
      "Accessed 660 posts\n",
      "Accessed 670 posts\n",
      "Accessed 680 posts\n",
      "Accessed 690 posts\n",
      "Accessed 700 posts\n",
      "Data saved at 700 posts\n",
      "Accessed 710 posts\n",
      "Accessed 720 posts\n",
      "Accessed 730 posts\n",
      "Accessed 740 posts\n",
      "Accessed 750 posts\n",
      "Accessed 760 posts\n",
      "Accessed 770 posts\n",
      "Accessed 780 posts\n",
      "Accessed 790 posts\n",
      "Accessed 800 posts\n",
      "Data saved at 800 posts\n",
      "Accessed 810 posts\n",
      "Accessed 820 posts\n",
      "Accessed 830 posts\n",
      "Accessed 840 posts\n",
      "Accessed 850 posts\n",
      "Accessed 860 posts\n",
      "Accessed 870 posts\n",
      "Accessed 880 posts\n",
      "Accessed 890 posts\n",
      "Accessed 900 posts\n",
      "Data saved at 900 posts\n",
      "Accessed 910 posts\n",
      "Accessed 920 posts\n",
      "Accessed 930 posts\n",
      "Accessed 940 posts\n",
      "Accessed 950 posts\n",
      "Accessed 960 posts\n",
      "Accessed 970 posts\n",
      "Accessed 980 posts\n",
      "Accessed 990 posts\n",
      "Succesfully scraped the data\n"
     ]
    }
   ],
   "source": [
    "top_posts = scrape_data(\"top\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc79f418-8e91-4d06-bca0-dbd028b4b2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessed 0 posts\n",
      "Accessed 10 posts\n",
      "Accessed 20 posts\n",
      "Accessed 30 posts\n",
      "Accessed 40 posts\n",
      "Accessed 50 posts\n",
      "Accessed 60 posts\n",
      "Accessed 70 posts\n",
      "Accessed 80 posts\n",
      "Accessed 90 posts\n",
      "Accessed 100 posts\n",
      "Data saved at 100 posts\n",
      "Accessed 110 posts\n",
      "Accessed 120 posts\n",
      "Accessed 130 posts\n",
      "Accessed 140 posts\n",
      "Accessed 150 posts\n",
      "Accessed 160 posts\n",
      "Accessed 170 posts\n",
      "Accessed 180 posts\n",
      "Accessed 190 posts\n",
      "Accessed 200 posts\n",
      "Data saved at 200 posts\n",
      "Accessed 210 posts\n",
      "Accessed 220 posts\n",
      "Accessed 230 posts\n",
      "Accessed 240 posts\n",
      "Accessed 250 posts\n",
      "Accessed 260 posts\n",
      "Accessed 270 posts\n",
      "Accessed 280 posts\n",
      "Accessed 290 posts\n",
      "Accessed 300 posts\n",
      "Data saved at 300 posts\n",
      "Accessed 310 posts\n",
      "Accessed 320 posts\n",
      "Accessed 330 posts\n",
      "Accessed 340 posts\n",
      "Accessed 350 posts\n",
      "Accessed 360 posts\n",
      "Accessed 370 posts\n",
      "Accessed 380 posts\n",
      "Accessed 390 posts\n",
      "Accessed 400 posts\n",
      "Data saved at 400 posts\n",
      "Accessed 410 posts\n",
      "Accessed 420 posts\n",
      "Accessed 430 posts\n",
      "Accessed 440 posts\n",
      "Accessed 450 posts\n",
      "Accessed 460 posts\n",
      "Accessed 470 posts\n",
      "Accessed 480 posts\n",
      "Accessed 490 posts\n",
      "Accessed 500 posts\n",
      "Data saved at 500 posts\n",
      "Accessed 510 posts\n",
      "Accessed 520 posts\n",
      "Accessed 530 posts\n",
      "Accessed 540 posts\n",
      "Accessed 550 posts\n",
      "Accessed 560 posts\n",
      "Accessed 570 posts\n",
      "Accessed 580 posts\n",
      "Accessed 590 posts\n",
      "Accessed 600 posts\n",
      "Data saved at 600 posts\n",
      "Accessed 610 posts\n",
      "Accessed 620 posts\n",
      "Accessed 630 posts\n",
      "Accessed 640 posts\n",
      "Accessed 650 posts\n",
      "Accessed 660 posts\n",
      "Accessed 670 posts\n",
      "Accessed 680 posts\n",
      "Accessed 690 posts\n",
      "Accessed 700 posts\n",
      "Data saved at 700 posts\n",
      "Accessed 710 posts\n",
      "Accessed 720 posts\n",
      "Accessed 730 posts\n",
      "Succesfully scraped the data\n"
     ]
    }
   ],
   "source": [
    "hot_posts = scrape_data(\"hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "a7c7fa5e-5c95-4010-a230-ee12634a874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('AskDoc_hot_data1.csv')\n",
    "# data1_copy = data1.copy()\n",
    "data2 = pd.read_csv('AskDoc_hot_data2.csv')\n",
    "# data2_copy = data2.copy()\n",
    "data3 = pd.read_csv('AskDoc_topdata.csv')\n",
    "# data3_copy = data3.copy()\n",
    "\n",
    "merged_data = pd.concat([data1, data2, data3], ignore_index = True)\n",
    "merged_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# print(merged_data)\n",
    "# print(len(merged_data))\n",
    "\n",
    "final_data = merged_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf960ff-e716-4d50-a725-60c8dbd40edb",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Notes for later:\n",
    "\n",
    "handle null data\n",
    "\n",
    "handle duplicates\n",
    "\n",
    "Removing mod comments\n",
    "\n",
    "Since hot posts may have unmoderated comment, can't just rely on reddit rules to assume the data's good. Thus, gotta use all sorts of methods to clean the posts and comments before fine tuning the llm. My plan:\n",
    "\n",
    "replace special characters\n",
    "\n",
    "replace html stuff\n",
    "\n",
    "replace encoded characters\n",
    "\n",
    "detect non english language and exclude them\n",
    "\n",
    "spell check\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "d78e4535-af9b-462f-b96e-9126f5bc6f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of rows for the scraped data: 6956\n"
     ]
    }
   ],
   "source": [
    "#null data handling\n",
    "print(f\"Initial number of rows for the scraped data: {len(final_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "25047614-20b1-4872-a86d-b1e516a59d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdi = MarkdownIt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "d662a397-089f-4b9d-857c-1bab1111e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, author):\n",
    "    # Skip processing for bot authors or deleted/removed content\n",
    "    if author in ('AskDocs-ModTeam', 'AutoModerator') or text in ('[removed]', '[deleted]'):\n",
    "        return None \n",
    "\n",
    "    emojis = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    \n",
    "    # Continue processing if not by a bot or deleted/removed\n",
    "    text = mdi.render(text)\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = html.unescape(text)\n",
    "    text = ' '.join(text.split())\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = emojis.sub(r'', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "final_data['title'] = final_data.apply(lambda row: clean_text(row['title'], row['author']), axis=1)\n",
    "final_data['selftext'] = final_data.apply(lambda row: clean_text(row['selftext'], row['author']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "42c59fb5-cc86-4a57-b69e-c389a658c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comments(comments, author):\n",
    "    comments_list = ast.literal_eval(comments)\n",
    "    cleaned_comments = []\n",
    "    for comment in comments_list:\n",
    "        if comment['author'] not in ('AskDocs-ModTeam', 'AutoModerator') and comment['body'] not in ('[removed]', '[deleted]'):\n",
    "            cleaned_text = clean_text(comment['body'], comment['author'])\n",
    "            if cleaned_text is not None:\n",
    "                cleaned_comments.append(cleaned_text)\n",
    "    return cleaned_comments\n",
    "\n",
    "final_data['comments'] = final_data.apply(lambda row: clean_comments(row['comments'], row['author']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "22424005-b891-4a37-a78e-676d6f580cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6956"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "52b985d7-c01f-4228-a80d-80a3d799bc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after dropping null values: 4777\n"
     ]
    }
   ],
   "source": [
    "new_data = final_data.dropna()\n",
    "# final_data = final_data[final_data['comments'].str.len() > 5]\n",
    "print(f\"Number of rows after dropping null values: {len(new_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "b82acaf0-0b03-437f-af31-b497001772f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv('CleanData.csv', encoding = 'utf-8', index = False)\n",
    "# new_data.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a5e08-5359-40fa-be4a-c60be234e562",
   "metadata": {},
   "source": [
    "## Data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a3265-c4c0-43b3-bba0-815f00561435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(comment):\n",
    "    return TextBlob(comment).sentiment.polarity\n",
    "\n",
    "# Apply sentiment analysis on a sample comment\n",
    "sample_sentiment = sentiment_analysis(final_data['comments'][500][1])  # Analyze the second comment for example\n",
    "print(\"Sentiment Polarity:\", sample_sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
